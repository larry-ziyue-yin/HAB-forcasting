{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73043a68",
   "metadata": {},
   "source": [
    "# Forecasting Freshwater Algal Bloom Levels Using Multisource Climate and Water-Quality Data\n",
    "\n",
    "*This is the course project of **STATS 402: Interdisciplinary Data Analysis**.*\n",
    "\n",
    "**Name:** Ziyue Yin\n",
    "\n",
    "**NetID:** zy166"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac208a0",
   "metadata": {},
   "source": [
    "## Dataset: HydroLAKES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ef2d1",
   "metadata": {},
   "source": [
    "Lake polygons (including all attributes) in shapefileformat: https://www.hydrosheds.org/products/hydrolakes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e9d58b",
   "metadata": {},
   "source": [
    "## Dataset: NASA OceanColor Inland Waters (ILW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920eaa6",
   "metadata": {},
   "source": [
    "S3Merged-ILW data: https://oceandata.sci.gsfc.nasa.gov/directdataaccess/Level-3%20Mapped/Merged-S3-ILW/.\n",
    "\n",
    "S3B-ILW data: https://oceandata.sci.gsfc.nasa.gov/directdataaccess/Level-3%20Mapped/S3B-ILW/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0419fb8",
   "metadata": {},
   "source": [
    "After downloading the datasets, the structure should be shown as follows:\n",
    "\n",
    "```\n",
    "datasets/\n",
    " ├── ILW/\n",
    " │    ├── S3B/2024/CONUS_MO/\n",
    " │    │      ├── S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    │      ├── S3B_OLCI_EFRNT.20240201_20240229.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    ├── Merged/2024/CONUS_DAY/\n",
    " │    │      ├── S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    │      ├── S3M_OLCI_EFRNT.20240102.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    │      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e70cade",
   "metadata": {},
   "source": [
    "### Data Structure Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e84bc5",
   "metadata": {},
   "source": [
    "First of all, let's glance at the monthly dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3545ae05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenMappingWarningOnValuesAccess({'y': 15138, 'x': 26328, 'rgb': 3, 'eightbitcolor': 256})\n",
      "['rhos_400', 'rhos_412', 'rhos_443', 'rhos_490', 'rhos_510', 'rhos_560', 'rhos_620', 'rhos_665', 'rhos_674', 'rhos_681', 'rhos_709', 'rhos_754', 'rhos_865', 'rhos_884', 'CI_cyano', 'palette']\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "p = \"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/S3B/2024/CONUS_MO/S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\"\n",
    "\n",
    "ds = xr.open_dataset(p, engine=\"netcdf4\", chunks=\"auto\")\n",
    "print(ds.dims)\n",
    "print(list(ds.data_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b94fb",
   "metadata": {},
   "source": [
    "And also, the daily dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a063b2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenMappingWarningOnValuesAccess({'y': 15138, 'x': 26328, 'rgb': 3, 'eightbitcolor': 256})\n",
      "['rhos_400', 'rhos_412', 'rhos_443', 'rhos_490', 'rhos_510', 'rhos_560', 'rhos_620', 'rhos_665', 'rhos_674', 'rhos_681', 'rhos_709', 'rhos_754', 'rhos_865', 'rhos_884', 'CI_cyano', 'palette']\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "p = \"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\"\n",
    "\n",
    "ds = xr.open_dataset(p, engine=\"netcdf4\", chunks=\"auto\")\n",
    "print(ds.dims)\n",
    "print(list(ds.data_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627b349",
   "metadata": {},
   "source": [
    "### Target Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edbba54",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8fe2bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import rioxarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb166a00",
   "metadata": {},
   "source": [
    "Time Extraction: coords -> attrs -> file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d9caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_time_label(nc_path, ds, product=\"monthly\"):\n",
    "    \"\"\"\n",
    "    Return a pandas.Timestamp, try to infer from ds or filename.\n",
    "    product: 'monthly' or 'daily'\n",
    "    \"\"\"\n",
    "    # 1) Directly have time coordinate/variable\n",
    "    for k in (\"time\",):\n",
    "        if k in ds.coords or k in ds.variables:\n",
    "            try:\n",
    "                return pd.to_datetime(ds[k].values).to_pydatetime()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # 2) Global attributes (common in L3M data)\n",
    "    start = ds.attrs.get(\"time_coverage_start\") or ds.attrs.get(\"start_time\")\n",
    "    end   = ds.attrs.get(\"time_coverage_end\")   or ds.attrs.get(\"end_time\")\n",
    "    if start and end:\n",
    "        try:\n",
    "            ts = pd.to_datetime(start)\n",
    "            te = pd.to_datetime(end)\n",
    "            if product == \"monthly\":\n",
    "                return ts + (te - ts) / 2\n",
    "            else:\n",
    "                return ts\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 3) Analyze the filename\n",
    "    fn = nc_path.split(\"/\")[-1]\n",
    "    if product == \"monthly\":\n",
    "        # ...YYYYMMDD_YYYYMMDD.L3m.MO...\n",
    "        m = re.search(r\"\\.(\\d{8})_(\\d{8})\\.L3m\\.MO\\.\", fn)\n",
    "        if m:\n",
    "            b, e = m.group(1), m.group(2)\n",
    "            ts = pd.to_datetime(b, format=\"%Y%m%d\")\n",
    "            te = pd.to_datetime(e, format=\"%Y%m%d\")\n",
    "            return ts + (te - ts) / 2\n",
    "    else:\n",
    "        # ...YYYYMMDD.L3m.DAY...\n",
    "        m = re.search(r\"\\.(\\d{8})\\.L3m\\.DAY\\.\", fn)\n",
    "        if m:\n",
    "            return pd.to_datetime(m.group(1), format=\"%Y%m%d\")\n",
    "\n",
    "    raise ValueError(\"Cannot infer time from dataset or filename: \" + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e9e64",
   "metadata": {},
   "source": [
    "Quality Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119622ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ci(da: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Filter out values out of physical range and remove near-zero values.\n",
    "    \"\"\"\n",
    "    vmin = float(da.attrs.get(\"valid_min\", np.nan))\n",
    "    vmax = float(da.attrs.get(\"valid_max\", np.nan))\n",
    "    if np.isfinite(vmin):\n",
    "        da = da.where(da >= vmin)\n",
    "    if np.isfinite(vmax):\n",
    "        da = da.where(da <= vmax)\n",
    "\n",
    "    thr = max(vmin, 5e-5) if np.isfinite(vmin) else 5e-5\n",
    "    da = da.where(da > thr)\n",
    "\n",
    "    return da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435bbce3",
   "metadata": {},
   "source": [
    "For a single .nc file, get all the lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d5d1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lakes_from_nc(nc_path: str,\n",
    "                          lakes_gdf: gpd.GeoDataFrame,\n",
    "                          lake_id_col: str,\n",
    "                          product: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    nc_path: A single NetCDF file (S3B monthly or S3M daily)\n",
    "    lakes_gdf: A GeoDataFrame containing `lake_id` and `geometry` (EPSG:4326)\n",
    "    product: 'monthly' | 'daily'\n",
    "    Returns: One row per lake (timestamp of the file)\n",
    "    \"\"\"\n",
    "    ds = xr.open_dataset(nc_path, engine=\"netcdf4\", chunks=\"auto\")\n",
    "    t  = infer_time_label(nc_path, ds, product=product)\n",
    "\n",
    "    da = ds[\"CI_cyano\"]\n",
    "    da = set_spatial_dims_safe(da)\n",
    "    da = clean_ci(da)\n",
    "\n",
    "    rows = []\n",
    "    for _, row in lakes_gdf.iterrows():\n",
    "        lid  = row[lake_id_col]\n",
    "        geom = [row.geometry]  # rioxarray.clip needs a list\n",
    "\n",
    "        try:\n",
    "            clipped = da.rio.clip(geom, lakes_gdf.crs, drop=True)\n",
    "            valid   = clipped.where(np.isfinite(clipped))\n",
    "            n_valid = int(valid.count().compute().values)\n",
    "            if n_valid == 0:\n",
    "                mean_val = np.nan\n",
    "                p90      = np.nan\n",
    "            else:\n",
    "                mean_val = float(valid.mean().compute().values)\n",
    "                p90      = float(valid.quantile(0.9).compute().values)\n",
    "        except Exception:\n",
    "            mean_val, p90, n_valid = np.nan, np.nan, 0\n",
    "\n",
    "        rows.append({\n",
    "            \"lake_id\": lid,\n",
    "            \"time\":   pd.to_datetime(t),\n",
    "            \"product\": product,\n",
    "            \"CI_mean\": mean_val,\n",
    "            \"CI_p90\":  p90,\n",
    "            \"n_valid\": n_valid,\n",
    "            \"src\":     Path(nc_path).name,\n",
    "        })\n",
    "\n",
    "    ds.close()\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d60dc4",
   "metadata": {},
   "source": [
    "Process monthly data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84adfb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_monthly(monthly_dir: str,\n",
    "                lakes_fp: str,\n",
    "                lake_id_col: str,\n",
    "                out_parquet: str):\n",
    "    \"\"\"\n",
    "    monthly_dir: Directory containing files like S3B_OLCI_EFRNT.*.L3m.MO.ILW_CONUS...nc\n",
    "    lakes_fp:    Lake boundaries (gpkg/shp, must be EPSG:4326)\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(lakes_fp)\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"The lake file is missing CRS, please ensure it is EPSG:4326\")\n",
    "    gdf = gdf.to_crs(4326)[[lake_id_col, \"geometry\"]].dropna()\n",
    "\n",
    "    out_rows = []\n",
    "    for fp in sorted(Path(monthly_dir).glob(\"S3B_OLCI_EFRNT.*.L3m.MO.*.nc\")):\n",
    "        df_one = extract_lakes_from_nc(str(fp), gdf, lake_id_col, product=\"monthly\")\n",
    "        out_rows.append(df_one)\n",
    "\n",
    "    if not out_rows:\n",
    "        print(\"No monthly files found.\")\n",
    "        return\n",
    "\n",
    "    df_all = pd.concat(out_rows, ignore_index=True)\n",
    "    Path(out_parquet).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_all.to_parquet(out_parquet, index=False)\n",
    "    print(f\"[monthly] saved → {out_parquet}  ({len(df_all)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a17ebf",
   "metadata": {},
   "source": [
    "Process daily data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47b24609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_daily(daily_dir: str,\n",
    "              lakes_fp: str,\n",
    "              lake_id_col: str,\n",
    "              out_parquet: str):\n",
    "    \"\"\"\n",
    "    daily_dir: Directory containing files like S3M_OLCI_EFRNT.*.L3m.DAY.ILW_CONUS...nc\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(lakes_fp)\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"The lake file is missing CRS, please ensure it is EPSG:4326\")\n",
    "    gdf = gdf.to_crs(4326)[[lake_id_col, \"geometry\"]].dropna()\n",
    "\n",
    "    out_rows = []\n",
    "    for fp in sorted(Path(daily_dir).glob(\"S3M_OLCI_EFRNT.*.L3m.DAY.*.nc\")):\n",
    "        df_one = extract_lakes_from_nc(str(fp), gdf, lake_id_col, product=\"daily\")\n",
    "        out_rows.append(df_one)\n",
    "\n",
    "    if not out_rows:\n",
    "        print(\"No daily files found.\")\n",
    "        return\n",
    "\n",
    "    df_all = pd.concat(out_rows, ignore_index=True)\n",
    "    Path(out_parquet).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_all.to_parquet(out_parquet, index=False)\n",
    "    print(f\"[daily] saved → {out_parquet}  ({len(df_all)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01691f18",
   "metadata": {},
   "source": [
    "Spatial Coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3e57e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_spatial_dims_safe(da: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Try to set the spatial dimensions and CRS for L3m grids.\n",
    "    First use `x/y`; if failed, try `lon/lat`; if still failed, degrade to the last two dimensions as `x/y`.\n",
    "    \"\"\"\n",
    "    if \"x\" in da.dims and \"y\" in da.dims:\n",
    "        out = da.rio.write_crs(4326)\n",
    "        out = out.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "        return out\n",
    "\n",
    "    if \"lon\" in da.dims and \"lat\" in da.dims:\n",
    "        out = da.rio.write_crs(4326)\n",
    "        out = out.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=False)\n",
    "        return out\n",
    "\n",
    "    if len(da.dims) >= 2:\n",
    "        dims = list(da.dims)\n",
    "        ydim, xdim = dims[-2], dims[-1]\n",
    "        out = da.rename({xdim: \"x\", ydim: \"y\"})\n",
    "        out = out.rio.write_crs(4326)\n",
    "        out = out.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "        return out\n",
    "\n",
    "    raise ValueError(\"Cannot determine spatial dims for CI_cyano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4665a61",
   "metadata": {},
   "source": [
    "#### Scale 1: In general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f52fe",
   "metadata": {},
   "source": [
    "##### Monthly\n",
    "\n",
    "Here, we use the **S3B Monthly** data. One month per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f98a86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>CI_mean</th>\n",
       "      <th>CI_p90</th>\n",
       "      <th>n_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-16 16:53:28.500000+00:00</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>2030281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-15 17:12:11.500000+00:00</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>2204191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-16 16:44:09+00:00</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>1810493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-16 04:52:02+00:00</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>2151149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-05-16 17:01:29+00:00</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>2586974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              time   CI_mean    CI_p90  n_valid\n",
       "0 2024-01-16 16:53:28.500000+00:00  0.000591  0.000472  2030281\n",
       "1 2024-02-15 17:12:11.500000+00:00  0.000539  0.000351  2204191\n",
       "2        2024-03-16 16:44:09+00:00  0.000492  0.000166  1810493\n",
       "3        2024-04-16 04:52:02+00:00  0.000496  0.000540  2151149\n",
       "4        2024-05-16 17:01:29+00:00  0.000500  0.000836  2586974"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob, numpy as np, pandas as pd, xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "monthly_dir = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/S3B/2024/CONUS_MO\")\n",
    "out_csv = monthly_dir/\"ci_cyano_monthly_mean.csv\"\n",
    "\n",
    "rows = []\n",
    "for fp in sorted(monthly_dir.glob(\"S3B_OLCI_EFRNT.*.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\")):\n",
    "    ds = xr.open_dataset(fp, engine=\"netcdf4\", chunks=\"auto\")\n",
    "    da = ds[\"CI_cyano\"]\n",
    "    da = clean_ci(da)\n",
    "\n",
    "    vmin = float(da.attrs.get(\"valid_min\", np.nan))\n",
    "    vmax = float(da.attrs.get(\"valid_max\", np.nan))\n",
    "    if np.isfinite(vmin): da = da.where(da >= vmin)\n",
    "    if np.isfinite(vmax): da = da.where(da <= vmax)\n",
    "\n",
    "    m   = float(da.where(np.isfinite(da)).mean().compute().values)\n",
    "    p90 = float(da.where(np.isfinite(da)).quantile(0.9).compute().values)\n",
    "    t   = infer_time_label(str(fp), ds, product=\"monthly\")\n",
    "\n",
    "    rows.append({\"time\": pd.to_datetime(t), \"CI_mean\": m, \"CI_p90\": p90,\n",
    "                 \"n_valid\": int(da.count().compute().values)})\n",
    "    ds.close()\n",
    "\n",
    "df_mo = pd.DataFrame(rows).sort_values(\"time\").reset_index(drop=True)\n",
    "df_mo.to_csv(out_csv, index=False)\n",
    "df_mo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b93b8d",
   "metadata": {},
   "source": [
    "##### Daily\n",
    "\n",
    "Here, we use the **S3M Daily** data. One day per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1970169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_dir = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/Merged/2024/CONUS_DAY\")\n",
    "# out_csv = daily_dir/\"ci_cyano_daily_mean.csv\"\n",
    "\n",
    "# rows = []\n",
    "# for fp in sorted(daily_dir.glob(\"S3M_OLCI_EFRNT.*.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\")):\n",
    "#     ds = xr.open_dataset(fp, engine=\"netcdf4\", chunks=\"auto\")\n",
    "#     da = ds[\"CI_cyano\"]\n",
    "#     da = clean_ci(da)\n",
    "\n",
    "#     vmin = float(da.attrs.get(\"valid_min\", np.nan))\n",
    "#     vmax = float(da.attrs.get(\"valid_max\", np.nan))\n",
    "#     if np.isfinite(vmin): da = da.where(da >= vmin)\n",
    "#     if np.isfinite(vmax): da = da.where(da <= vmax)\n",
    "\n",
    "#     m   = float(da.where(np.isfinite(da)).mean().compute().values)\n",
    "#     p90 = float(da.where(np.isfinite(da)).quantile(0.9).compute().values)\n",
    "#     t   = infer_time_label(str(fp), ds, product=\"daily\")\n",
    "\n",
    "#     rows.append({\"date\": pd.to_datetime(t), \"CI_mean\": m, \"CI_p90\": p90,\n",
    "#                  \"n_valid\": int(da.count().compute().values)})\n",
    "#     ds.close()\n",
    "\n",
    "# df_day = pd.DataFrame(rows).sort_values(\"date\").reset_index(drop=True)\n",
    "# df_day.to_csv(out_csv, index=False)\n",
    "# df_day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4fc325",
   "metadata": {},
   "source": [
    "#### Scale 2: Five Great Lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b2c1f",
   "metadata": {},
   "source": [
    "First of all, we get the five Great Lakes out explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f6808a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /dkucc/home/zy166/HAB-forcasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg features: 5\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "\n",
    "src = \"/dkucc/home/zy166/HAB-forcasting/datasets/Lakes/shapes/lakes_greatlakes.gpkg\"\n",
    "gdf = gpd.read_file(src)\n",
    "\n",
    "keep_names = [\"Superior\",\"Michigan\",\"Huron\",\"Erie\",\"Ontario\"]\n",
    "gdf5 = gdf[gdf[\"Lake_name\"].str.fullmatch(\"|\".join(keep_names), case=False)].copy()\n",
    "\n",
    "# Dissolve into five polygons\n",
    "gdf5 = gdf5.dissolve(by=\"Lake_name\", as_index=False)\n",
    "\n",
    "# Clean and rename the columns\n",
    "gdf5 = gdf5.rename(columns={\"Lake_name\":\"lake_name\"})\n",
    "gdf5 = gdf5.reset_index(drop=True)\n",
    "\n",
    "# Directly overwrite/create the `lake_id` column\n",
    "gdf5[\"lake_id\"] = [f\"GL-{i+1}\" for i in range(len(gdf5))]   # GL-1..GL-5\n",
    "\n",
    "# Buffer the shoreline by 300 m, first to equidistant projection, then buffer, then back to 4326\n",
    "gdf5m = gdf5.to_crs(5070)\n",
    "gdf5m[\"geometry\"] = gdf5m.buffer(-300)\n",
    "gdf5 = gdf5m.to_crs(4326)\n",
    "\n",
    "out = Path(src).with_name(\"lakes_greatlakes_5poly.gpkg\")\n",
    "gdf5.to_file(out, driver=\"GPKG\")\n",
    "print(\"Saved:\", out, \"features:\", len(gdf5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843c8b2d",
   "metadata": {},
   "source": [
    "##### Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb3e1315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[monthly] saved → /dkucc/home/zy166/HAB-forcasting/datasets/processed/lake_ci_monthly.parquet  (55 rows)\n"
     ]
    }
   ],
   "source": [
    "# Monthly\n",
    "run_monthly(\n",
    "    monthly_dir=\"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/S3B/2024/CONUS_MO\",\n",
    "    lakes_fp=\"/dkucc/home/zy166/HAB-forcasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\",\n",
    "    lake_id_col=\"lake_id\",\n",
    "    out_parquet=\"/dkucc/home/zy166/HAB-forcasting/datasets/processed/lake_ci_monthly.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c23b3",
   "metadata": {},
   "source": [
    "##### Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ea7c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily\n",
    "# run_daily(\n",
    "#     daily_dir=\"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/Merged/2024/CONUS_DAY\",\n",
    "#     lakes_fp=\"/dkucc/home/zy166/HAB-forcasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\",\n",
    "#     lake_id_col=\"lake_id\",\n",
    "#     out_parquet=\"/dkucc/home/zy166/HAB-forcasting/data/processed/lake_ci_daily.parquet\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25773dfe",
   "metadata": {},
   "source": [
    "### Data Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a47a60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['lake_id', 'product', 'CI_mean', 'CI_p90', 'n_valid', 'src', 'date', 'area_m2', 'expected_pixels_geom', 'lake_name', 'empiric_n_valid_max', 'pct_valid_geom', 'pct_valid_emp', 'qc_low_cov_geom', 'qc_low_cov_emp', 'qc_tiny_abs_pix', 'qc_is_valid']\n",
      "Shape: (55, 17)\n",
      "  lake_id  product  CI_mean  CI_p90  n_valid  \\\n",
      "0    GL-1  monthly      NaN     NaN      NaN   \n",
      "1    GL-2  monthly      NaN     NaN      NaN   \n",
      "2    GL-3  monthly      NaN     NaN      NaN   \n",
      "3    GL-4  monthly      NaN     NaN      NaN   \n",
      "4    GL-5  monthly      NaN     NaN      NaN   \n",
      "\n",
      "                                                 src                    date  \\\n",
      "0  S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CO... 2024-01-16 16:53:28.500   \n",
      "1  S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CO... 2024-01-16 16:53:28.500   \n",
      "2  S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CO... 2024-01-16 16:53:28.500   \n",
      "3  S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CO... 2024-01-16 16:53:28.500   \n",
      "4  S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CO... 2024-01-16 16:53:28.500   \n",
      "\n",
      "        area_m2  expected_pixels_geom lake_name  empiric_n_valid_max  \\\n",
      "0  2.488940e+10                276549      Erie                  NaN   \n",
      "1  5.581964e+10                620218     Huron                  NaN   \n",
      "2  5.620858e+10                624540  Michigan                  NaN   \n",
      "3  1.815035e+10                201671   Ontario                  NaN   \n",
      "4  7.928849e+10                880983  Superior                  NaN   \n",
      "\n",
      "   pct_valid_geom  pct_valid_emp  qc_low_cov_geom  qc_low_cov_emp  \\\n",
      "0            <NA>            NaN             <NA>            <NA>   \n",
      "1            <NA>            NaN             <NA>            <NA>   \n",
      "2            <NA>            NaN             <NA>            <NA>   \n",
      "3            <NA>            NaN             <NA>            <NA>   \n",
      "4            <NA>            NaN             <NA>            <NA>   \n",
      "\n",
      "   qc_tiny_abs_pix  qc_is_valid  \n",
      "0                0            1  \n",
      "1                0            1  \n",
      "2                0            1  \n",
      "3                0            1  \n",
      "4                0            1  \n",
      "CI_mean: non-null=0, finite=0, mean=nan\n",
      "CI_p90: non-null=0, finite=0, mean=nan\n",
      "n_valid: non-null=0, finite=0, mean=nan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_parquet(\"/dkucc/home/zy166/HAB-forcasting/datasets/processed/qc/greatlakes_monthly_clean.parquet\")\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "for c in [\"CI_mean\", \"CI_p90\", \"n_valid\"]:\n",
    "    if c in df.columns:\n",
    "        v = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        print(f\"{c}: non-null={v.notna().sum()}, finite={np.isfinite(v).sum()}, mean={v.mean() if np.isfinite(v).any() else np.nan}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7f4ba19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved regional QC → /dkucc/home/zy166/HAB-forcasting/datasets/processed/qc/conus_daily_clean.csv\n",
      "[OK] Saved regional QC → /dkucc/home/zy166/HAB-forcasting/datasets/processed/qc/conus_monthly_clean.csv\n",
      "[SKIP] Missing: /dkucc/home/zy166/HAB-forcasting/datasets/processed/lake_ci_daily.parquet\n",
      "[WARN] n_valid is missing or all zeros in lake parquet; set n_valid = NaN so coverage flags become NA.\n",
      "[OK] Saved lake-level QC → /dkucc/home/zy166/HAB-forcasting/datasets/processed/qc/greatlakes_monthly_clean.parquet\n",
      "[OK] Summary → /dkucc/home/zy166/HAB-forcasting/datasets/processed/qc/greatlakes_monthly_summary.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "QC & Completeness checks for NASA ILW Cyanobacteria Index (CI_cyano)\n",
    "- Regional (CONUS) daily & monthly time series\n",
    "- Lake-level (Great Lakes) daily & monthly time series\n",
    "\n",
    "Inputs (already prepared by you):\n",
    "1) /dkucc/home/zy166/HAB-forcasting/datasets/ILW/Merged/2024/CONUS_DAY/ci_cyano_daily_mean.csv\n",
    "2) /dkucc/home/zy166/HAB-forcasting/datasets/ILW/S3B/2024/CONUS_MO/ci_cyano_monthly_mean.csv\n",
    "3) /dkucc/home/zy166/HAB-forcasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\n",
    "4) /dkucc/home/zy166/HAB-forcasting/datasets/processed/lake_ci_monthly.parquet\n",
    "5) /dkucc/home/zy166/HAB-forcasting/datasets/processed/lake_ci_daily.parquet\n",
    "\n",
    "Outputs:\n",
    "- Cleaned regional daily/monthly CSVs with QC flags\n",
    "- Cleaned lake-level daily/monthly Parquet with QC flags and completeness metrics\n",
    "- Simple summary CSVs per product (row counts, missing rates, clipping, etc.)\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# ----------------------------\n",
    "# Parameters (tune as needed)\n",
    "# ----------------------------\n",
    "# Resolution of ILW L3m grid (meters)\n",
    "PIX_RES_M = 300.0\n",
    "\n",
    "# Minimal valid pixel ratio when judged by geometry (n_valid / expected_pixels)\n",
    "MIN_PCT_VALID_GEOM = 0.10   # drop if coverage < 10%\n",
    "\n",
    "# Minimal absolute valid pixels per lake-time\n",
    "MIN_ABS_PIX = 50            # drop very tiny coverage\n",
    "\n",
    "# Empirical coverage threshold relative to best observed coverage for that lake\n",
    "MIN_PCT_VALID_EMP = 0.10    # drop if n_valid < 10% of lake's max observed n_valid\n",
    "\n",
    "# CI cleaning thresholds\n",
    "NEAR_ZERO_THRESHOLD = 5e-5  # drop near-zero values (already applied in earlier step, kept as doc)\n",
    "CLIP_QUANTILE_LOW  = 0.001  # lower clip for outliers\n",
    "CLIP_QUANTILE_HIGH = 0.999  # upper clip for outliers\n",
    "\n",
    "# Interpolation limits for regional (optional smoothing to fill short gaps)\n",
    "INTERP_LIMIT_DAYS = 3\n",
    "\n",
    "# ----------------------------\n",
    "# Paths\n",
    "# ----------------------------\n",
    "P_CONUS_DAILY   = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/Merged/2024/CONUS_DAY/ci_cyano_daily_mean.csv\")\n",
    "P_CONUS_MONTHLY = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/S3B/2024/CONUS_MO/ci_cyano_monthly_mean.csv\")\n",
    "P_LAKES_GPKG    = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\")\n",
    "P_LAKE_DAILY    = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/processed/lake_ci_daily.parquet\")\n",
    "P_LAKE_MONTHLY  = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/processed/lake_ci_monthly.parquet\")\n",
    "\n",
    "OUT_DIR = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/processed/qc\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "def _clip_series_q(s: pd.Series, qlow=CLIP_QUANTILE_LOW, qhigh=CLIP_QUANTILE_HIGH) -> pd.Series:\n",
    "    \"\"\"Clip a numeric series by quantiles; preserve NaNs.\"\"\"\n",
    "    if s.dropna().empty:\n",
    "        return s\n",
    "    lo = s.quantile(qlow)\n",
    "    hi = s.quantile(qhigh)\n",
    "    return s.clip(lower=lo, upper=hi)\n",
    "\n",
    "def _ensure_datetime(df: pd.DataFrame,\n",
    "                     prefer_cols=(\"date\", \"time\")) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust datetime parsing:\n",
    "    - try 'date' then 'time' (or any column that contains these names)\n",
    "    - use pandas 'mixed' parser and coerce to UTC tz-aware -> drop tz\n",
    "    - drop NaT rows\n",
    "    \"\"\"\n",
    "    # find a candidate column\n",
    "    col = None\n",
    "    for cand in prefer_cols:\n",
    "        if cand in df.columns:\n",
    "            col = cand\n",
    "            break\n",
    "    if col is None:\n",
    "        # try fuzzy match by column name\n",
    "        for c in df.columns:\n",
    "            lc = str(c).lower()\n",
    "            if \"date\" in lc or \"time\" in lc or \"datetime\" in lc or \"timestamp\" in lc:\n",
    "                col = c\n",
    "                break\n",
    "    if col is None:\n",
    "        raise ValueError(\"No recognizable datetime column (expect 'date'/'time').\")\n",
    "\n",
    "    # mixed-format tolerant parsing; force UTC then remove tz\n",
    "    # errors='coerce' will set unparsable entries to NaT (we drop them)\n",
    "    dt = pd.to_datetime(df[col], utc=True, format=\"mixed\", errors=\"coerce\")\n",
    "    dt = dt.dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = dt\n",
    "    # drop rows that failed to parse\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=[\"date\"]).reset_index(drop=True)\n",
    "    if len(df) < before:\n",
    "        print(f\"[QC] Dropped {before - len(df)} rows with unparseable datetime in column '{col}'\")\n",
    "    return df\n",
    "\n",
    "def _summarize_basic(df: pd.DataFrame, tag: str) -> pd.DataFrame:\n",
    "    \"\"\"Basic counts and missingness summary for quick logging.\"\"\"\n",
    "    out = {\n",
    "        \"tag\": tag,\n",
    "        \"rows\": len(df),\n",
    "        \"n_missing_CI_mean\": int(df[\"CI_mean\"].isna().sum()) if \"CI_mean\" in df.columns else None,\n",
    "        \"n_missing_CI_p90\":  int(df[\"CI_p90\"].isna().sum()) if \"CI_p90\" in df.columns else None,\n",
    "        \"n_missing_n_valid\": int(df[\"n_valid\"].isna().sum()) if \"n_valid\" in df.columns else None,\n",
    "    }\n",
    "    return pd.DataFrame([out])\n",
    "\n",
    "# ----------------------------\n",
    "# QC for regional (CONUS) time series\n",
    "# ----------------------------\n",
    "def qc_conus_timeseries(csv_path: Path, freq: str, out_prefix: str) -> None:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    expected = {\"CI_mean\", \"CI_p90\", \"n_valid\"}\n",
    "    missing = expected - set(df.columns)\n",
    "    if missing:\n",
    "        print(f\"[WARN] {csv_path.name} is missing columns: {missing}\")\n",
    "\n",
    "    # robust parse + drop NaT\n",
    "    df = _ensure_datetime(df, prefer_cols=(\"date\", \"time\"))\n",
    "\n",
    "    # normalize timestamps to start-of-period\n",
    "    if freq.upper() == \"D\":\n",
    "        df[\"date\"] = df[\"date\"].dt.floor(\"D\")\n",
    "    elif freq.upper() == \"MS\":\n",
    "        # start-of-month via how='start'\n",
    "        df[\"date\"] = df[\"date\"].dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "    else:\n",
    "        # fallback: floor to given freq\n",
    "        df[\"date\"] = df[\"date\"].dt.to_period(freq).dt.start_time\n",
    "\n",
    "    df = df.sort_values(\"date\").drop_duplicates(subset=[\"date\"]).reset_index(drop=True)\n",
    "\n",
    "    # build regular index over the observed span\n",
    "    idx = pd.date_range(df[\"date\"].min(), df[\"date\"].max(), freq=freq)\n",
    "    df = df.set_index(\"date\").reindex(idx).rename_axis(\"date\").reset_index()\n",
    "\n",
    "    # short-gap interpolation on CI columns\n",
    "    for c in [\"CI_mean\", \"CI_p90\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].interpolate(limit=INTERP_LIMIT_DAYS if freq.upper()==\"D\" else 1)\n",
    "\n",
    "    # robust quantile clipping\n",
    "    for c in [\"CI_mean\", \"CI_p90\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = _clip_series_q(df[c])\n",
    "\n",
    "    if \"n_valid\" in df.columns:\n",
    "        nv_thr = df[\"n_valid\"].quantile(0.05) if df[\"n_valid\"].notna().any() else 0\n",
    "        df[\"qc_low_coverage\"] = (df[\"n_valid\"] < nv_thr).astype(int)\n",
    "\n",
    "    out_csv = OUT_DIR / f\"{out_prefix}_clean.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    _summarize_basic(df, tag=f\"{out_prefix}\").to_csv(OUT_DIR / f\"{out_prefix}_summary.csv\", index=False)\n",
    "    print(f\"[OK] Saved regional QC → {out_csv}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Lake geometry → expected pixel count\n",
    "# ----------------------------\n",
    "def compute_expected_pixels_per_lake(lakes_gpkg: Path,\n",
    "                                     inset_m: float = 300.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute expected pixel counts per lake from geometry area, after optional inward buffer\n",
    "    to reduce shoreline contamination. Done in equal-area CRS (EPSG:5070) then back to 4326.\n",
    "\n",
    "    Returns a DataFrame: lake_id, lake_name (if present), expected_pixels_geom, area_m2\n",
    "    \"\"\"\n",
    "    g = gpd.read_file(lakes_gpkg)\n",
    "    # Normalize lake_id and names\n",
    "    cols = list(g.columns)\n",
    "    if \"lake_id\" not in cols:\n",
    "        raise ValueError(\"Expect 'lake_id' in lakes_greatlakes_5poly.gpkg\")\n",
    "\n",
    "    name_col = None\n",
    "    for cand in [\"lake_name\", \"Lake_name\", \"name\", \"Name\"]:\n",
    "        if cand in cols:\n",
    "            name_col = cand\n",
    "            break\n",
    "\n",
    "    # Reproject to equal-area CRS for stable buffering & area\n",
    "    gm = g.to_crs(5070)\n",
    "\n",
    "    # Optional inward buffer to avoid shoreline pixels\n",
    "    if inset_m and inset_m > 0:\n",
    "        gm[\"geometry\"] = gm.buffer(-abs(inset_m))\n",
    "\n",
    "    gm[\"area_m2\"] = gm.geometry.area\n",
    "    # Expected pixel count at given resolution\n",
    "    gm[\"expected_pixels_geom\"] = (gm[\"area_m2\"] / (PIX_RES_M ** 2)).round().astype(\"Int64\")\n",
    "\n",
    "    df = gm.to_crs(4326)[[\"lake_id\", \"area_m2\", \"expected_pixels_geom\"]].copy()\n",
    "    if name_col:\n",
    "        df[name_col] = g[name_col].values\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# QC for lake-level time series\n",
    "# ----------------------------\n",
    "def _safe_nanmedian(arr) -> float:\n",
    "    \"\"\"Return NaN if all values are NaN or array empty; otherwise nanmedian.\"\"\"\n",
    "    a = pd.Series(arr).astype(float)\n",
    "    a = a[np.isfinite(a)]\n",
    "    return float(np.nanmedian(a)) if len(a) else float(\"nan\")\n",
    "\n",
    "def _coerce_n_valid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 1) 如果已有 n_valid 且存在非零值，直接返回\n",
    "    if \"n_valid\" in df.columns and (df[\"n_valid\"].fillna(0) > 0).any():\n",
    "        return df\n",
    "\n",
    "    # 2) 常见别名兜底\n",
    "    for alias in [\"valid_count\", \"count_valid\", \"num_valid\", \"n\", \"count\"]:\n",
    "        if alias in df.columns and (df[alias].fillna(0) > 0).any():\n",
    "            df = df.rename(columns={alias: \"n_valid\"})\n",
    "            return df\n",
    "\n",
    "    # 3) 若 n_valid 不存在或全为 0 → 改成 NaN，避免误触发覆盖率判定\n",
    "    if \"n_valid\" not in df.columns or not (df[\"n_valid\"].fillna(0) > 0).any():\n",
    "        df[\"n_valid\"] = np.nan\n",
    "        print(\"[WARN] n_valid is missing or all zeros in lake parquet; \"\n",
    "              \"set n_valid = NaN so coverage flags become NA.\")\n",
    "    return df\n",
    "\n",
    "def qc_lake_timeseries(parquet_path: Path, lakes_gpkg: Path, out_prefix: str) -> None:\n",
    "    \"\"\"\n",
    "    QC for lake-level CI time series (daily or monthly parquet).\n",
    "    Adds:\n",
    "      - expected_pixels based on lake geometry\n",
    "      - pct_valid_geom = n_valid / expected_pixels\n",
    "      - pct_valid_emp = n_valid / max(n_valid) per lake\n",
    "      - QC flags: low coverage (geom/emp), tiny absolute pixels, and clipped values\n",
    "    Outputs cleaned Parquet + summary CSV.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    df = _coerce_n_valid(df)\n",
    "    # print(\"== Quick diag ==\")\n",
    "    # print(\"df cols:\", list(df.columns))\n",
    "    # print(\"unique lake_id in df (first 10):\", pd.Series(df[\"lake_id\"]).drop_duplicates().head(10).tolist())\n",
    "    # geom_df = compute_expected_pixels_per_lake(lakes_gpkg, inset_m=PIX_RES_M)\n",
    "    # print(\"unique lake_id in geom (first 10):\", pd.Series(geom_df[\"lake_id\"]).drop_duplicates().head(10).tolist())\n",
    "    # print(\"n_valid present?\", \"n_valid\" in df.columns, \n",
    "    #     \"; nonzero n_valid rows:\", int((df.get(\"n_valid\", pd.Series(dtype=float)) > 0).sum()))\n",
    "\n",
    "    # Robust datetime parse: prefer 'date', then 'time'\n",
    "    df = _ensure_datetime(df, prefer_cols=(\"date\", \"time\"))\n",
    "\n",
    "    # Ensure we end up with exactly ONE datetime column named 'date'\n",
    "    if \"time\" in df.columns and \"date\" in df.columns:\n",
    "        # we already parsed into df[\"date\"], so drop the raw 'time' column\n",
    "        df = df.drop(columns=[\"time\"])\n",
    "    elif \"time\" in df.columns and \"date\" not in df.columns:\n",
    "        # no 'date' yet — rename 'time' -> 'date'\n",
    "        df = df.rename(columns={\"time\": \"date\"})\n",
    "    # else: only 'date' existed — nothing to do\n",
    "\n",
    "    # (optional) guard: drop any accidental duplicate columns by keeping first\n",
    "    if df.columns.duplicated().any():\n",
    "        dup = df.columns[df.columns.duplicated()].tolist()\n",
    "        print(f\"[QC] Dropping duplicated columns: {dup}\")\n",
    "        df = df.loc[:, ~df.columns.duplicated()].copy()\n",
    "\n",
    "    # Ensure a 'product' column exists\n",
    "    if \"product\" not in df.columns:\n",
    "        df[\"product\"] = \"unknown\"\n",
    "\n",
    "    # Expected pixels per lake from geometry\n",
    "    geom_df = compute_expected_pixels_per_lake(lakes_gpkg, inset_m=PIX_RES_M)  # inset = 1 pixel\n",
    "    name_col = \"lake_name\" if \"lake_name\" in geom_df.columns else (\"Lake_name\" if \"Lake_name\" in geom_df.columns else None)\n",
    "\n",
    "    # Merge expected pixels\n",
    "    merge_cols = [\"lake_id\"]\n",
    "    df = df.merge(geom_df, on=\"lake_id\", how=\"left\")\n",
    "\n",
    "    # Empirical max n_valid per lake (skip zeros/NaNs)\n",
    "    if \"n_valid\" in df.columns:\n",
    "        # Only consider strictly positive counts to define an empirical max\n",
    "        emp_max = (\n",
    "            df.loc[df[\"n_valid\"].fillna(0) > 0]\n",
    "            .groupby(\"lake_id\")[\"n_valid\"]\n",
    "            .max()\n",
    "            .rename(\"empiric_n_valid_max\")\n",
    "        )\n",
    "        df = df.merge(emp_max, on=\"lake_id\", how=\"left\")\n",
    "\n",
    "        # Coverage metrics\n",
    "        # pct_valid_geom: finite only when expected_pixels_geom > 0\n",
    "        df[\"pct_valid_geom\"] = df[\"n_valid\"] / df[\"expected_pixels_geom\"]\n",
    "        # pct_valid_emp: finite only when empiric_n_valid_max > 0\n",
    "        denom = df[\"empiric_n_valid_max\"]\n",
    "        df[\"pct_valid_emp\"] = df[\"n_valid\"] / denom\n",
    "        # clean up invalid values\n",
    "        for c in (\"pct_valid_geom\", \"pct_valid_emp\"):\n",
    "            if c in df.columns:\n",
    "                df.loc[~np.isfinite(df[c]), c] = np.nan\n",
    "\n",
    "        # QC coverage flags（仅当可计算时才打标，否则置 NA）\n",
    "        df[\"qc_low_cov_geom\"] = pd.Series(pd.NA, index=df.index, dtype=\"Int64\")\n",
    "        ok_geom = df[\"pct_valid_geom\"].notna()\n",
    "        df.loc[ok_geom, \"qc_low_cov_geom\"] = (df.loc[ok_geom, \"pct_valid_geom\"] < MIN_PCT_VALID_GEOM).astype(\"Int64\")\n",
    "\n",
    "        df[\"qc_low_cov_emp\"] = pd.Series(pd.NA, index=df.index, dtype=\"Int64\")\n",
    "        ok_emp = df[\"pct_valid_emp\"].notna()\n",
    "        df.loc[ok_emp, \"qc_low_cov_emp\"] = (df.loc[ok_emp, \"pct_valid_emp\"] < MIN_PCT_VALID_EMP).astype(\"Int64\")\n",
    "\n",
    "        df[\"qc_tiny_abs_pix\"] = (df[\"n_valid\"] < MIN_ABS_PIX).astype(\"Int64\")\n",
    "    else:\n",
    "        for c in [\"pct_valid_geom\", \"pct_valid_emp\", \"qc_low_cov_geom\", \"qc_low_cov_emp\", \"qc_tiny_abs_pix\"]:\n",
    "            df[c] = pd.NA\n",
    "\n",
    "    # Clip CI columns lake-wise (robust to per-lake distributions)\n",
    "    for c in [\"CI_mean\", \"CI_p90\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df.groupby(\"lake_id\", group_keys=False)[c].apply(\n",
    "                lambda s: _clip_series_q(s, CLIP_QUANTILE_LOW, CLIP_QUANTILE_HIGH)\n",
    "            )\n",
    "\n",
    "    # A consolidated QC validity flag\n",
    "    # Valid if NOT any of (low cov geom, low cov emp, tiny abs pix)\n",
    "    cov_flags = [\"qc_low_cov_geom\", \"qc_low_cov_emp\", \"qc_tiny_abs_pix\"]\n",
    "    for c in cov_flags:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "\n",
    "    # start as valid\n",
    "    df[\"qc_is_valid\"] = 1\n",
    "\n",
    "    # any hard-fail coverage flag → invalid\n",
    "    for c in cov_flags:\n",
    "        df.loc[df[c] == 1, \"qc_is_valid\"] = 0\n",
    "\n",
    "    # rows with no coverage info at all → fall back to \"CI_mean is finite\"\n",
    "    rows_no_cov = df[cov_flags].isna().all(axis=1)\n",
    "    ci_ok = pd.to_numeric(df.get(\"CI_mean\"), errors=\"coerce\").notna().astype(int)\n",
    "    df.loc[rows_no_cov, \"qc_is_valid\"] = ci_ok.loc[rows_no_cov].values\n",
    "\n",
    "    # Save cleaned parquet\n",
    "    out_pq = OUT_DIR / f\"{out_prefix}_clean.parquet\"\n",
    "    df.to_parquet(out_pq, index=False)\n",
    "\n",
    "    # Build simple summary per-lake\n",
    "    summary_rows = []\n",
    "    for lid, g in df.groupby(\"lake_id\"):\n",
    "        n_rows = len(g)\n",
    "        n_valid_rows = int((g[\"qc_is_valid\"] == 1).sum())\n",
    "        frac_valid = n_valid_rows / n_rows if n_rows else 0.0\n",
    "        row = {\n",
    "            \"lake_id\": lid,\n",
    "            \"rows\": n_rows,\n",
    "            \"rows_valid\": n_valid_rows,\n",
    "            \"frac_valid\": round(frac_valid, 4),\n",
    "            \"pct_valid_geom_med\": _safe_nanmedian(g[\"pct_valid_geom\"]) if \"pct_valid_geom\" in g.columns else np.nan,\n",
    "            \"pct_valid_emp_med\":  _safe_nanmedian(g[\"pct_valid_emp\"])  if \"pct_valid_emp\"  in g.columns else np.nan,\n",
    "        }\n",
    "        if name_col and name_col in g.columns:\n",
    "            row[\"lake_name\"] = g[name_col].iloc[0]\n",
    "        summary_rows.append(row)\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows).sort_values(\"lake_id\")\n",
    "    out_summary = OUT_DIR / f\"{out_prefix}_summary.csv\"\n",
    "    summary_df.to_csv(out_summary, index=False)\n",
    "\n",
    "    print(f\"[OK] Saved lake-level QC → {out_pq}\")\n",
    "    print(f\"[OK] Summary → {out_summary}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Regional time series QC\n",
    "    if P_CONUS_DAILY.exists():\n",
    "        qc_conus_timeseries(P_CONUS_DAILY,   freq=\"D\",  out_prefix=\"conus_daily\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Missing: {P_CONUS_DAILY}\")\n",
    "\n",
    "    if P_CONUS_MONTHLY.exists():\n",
    "        qc_conus_timeseries(P_CONUS_MONTHLY, freq=\"MS\", out_prefix=\"conus_monthly\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Missing: {P_CONUS_MONTHLY}\")\n",
    "\n",
    "    # 2) Lake-level QC (Great Lakes)\n",
    "    if P_LAKE_DAILY.exists():\n",
    "        qc_lake_timeseries(P_LAKE_DAILY,  P_LAKES_GPKG, out_prefix=\"greatlakes_daily\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Missing: {P_LAKE_DAILY}\")\n",
    "\n",
    "    if P_LAKE_MONTHLY.exists():\n",
    "        qc_lake_timeseries(P_LAKE_MONTHLY, P_LAKES_GPKG, out_prefix=\"greatlakes_monthly\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Missing: {P_LAKE_MONTHLY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254a9d3",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "Available at independent python file `HAB-forcasting/data_visualization_ILW.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9fdb30",
   "metadata": {},
   "source": [
    "## Dataset: Daymet v4 Daily Surface Weather Data (ORNL DAAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13671ac",
   "metadata": {},
   "source": [
    "Read Great Lakes Polygon data (GPKG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1aa2065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pydaymet as daymet\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "GPKG = \"/dkucc/home/zy166/HAB-forcasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\"\n",
    "\n",
    "lakes = gpd.read_file(GPKG)\n",
    "# 确保是经纬度坐标（EPSG:4326）；PyDaymet也支持其它CRS，传参loc_crs即可\n",
    "lakes = lakes.to_crs(4326)\n",
    "\n",
    "# 准备日期与变量清单\n",
    "# DATES = (\"2016-01-01\", \"2025-12-31\")\n",
    "DATES = (\"2024-01-01\", \"2024-12-31\")\n",
    "VARS  = [\"tmin\",\"tmax\",\"prcp\",\"srad\",\"vp\",\"dayl\"]\n",
    "\n",
    "# 如果没有湖名列，就造一个 id\n",
    "if \"name\" not in lakes.columns:\n",
    "    lakes[\"name\"] = [f\"lake_{i}\" for i in range(len(lakes))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19464b83",
   "metadata": {},
   "source": [
    "逐湖请求 Daymet 网格并做“湖面平均”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c158faa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ServiceError",
     "evalue": "Service returned the following error message:\nURL: https://thredds.daac.ornl.gov/thredds/ncss/ornldaac/2129/daymet_v4_daily_na_vp_2024.nc?var=vp&north=42.902360&west=-83.475523&east=-78.857244&south=41.383397&disableProjSubset=on&horizStride=1&time_start=2024-01-01T12%3A00%3A00Z&time_end=2024-12-30T12%3A00%3A00Z&timeStride=1&addLatLon=true&accept=netcdf\nERROR: 400, message='', url='https://opendap.earthdata.nasa.gov/thredds/ncss/ornldaac/2129/daymet_v4_daily_na_dayl_2024.nc?var=dayl&north=42.902360&west=-83.475523&east=-78.857244&south=41.383397&disableProjSubset=on&horizStride=1&time_start=2024-01-01T12:00:00Z&time_end=2024-12-30T12:00:00Z&timeStride=1&addLatLon=true&accept=netcdf'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientResponseError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hab/lib/python3.11/site-packages/tiny_retriever/tiny_retriever.py:139\u001b[39m, in \u001b[36m_stream_file\u001b[39m\u001b[34m(session, url, filepath, chunk_size, raise_status, timeout)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session.get(url, timeout=ClientTimeout(timeout)) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[32m    140\u001b[39m         remote_size = \u001b[38;5;28mint\u001b[39m(response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Length\u001b[39m\u001b[33m\"\u001b[39m, -\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hab/lib/python3.11/site-packages/aiohttp/client.py:1510\u001b[39m, in \u001b[36m_BaseRequestContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1509\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> _RetType:\n\u001b[32m-> \u001b[39m\u001b[32m1510\u001b[39m     \u001b[38;5;28mself\u001b[39m._resp: _RetType = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._coro\n\u001b[32m   1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._resp.\u001b[34m__aenter__\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hab/lib/python3.11/site-packages/aiohttp/client.py:906\u001b[39m, in \u001b[36mClientSession._request\u001b[39m\u001b[34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size, middlewares)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m raise_for_status:\n\u001b[32m--> \u001b[39m\u001b[32m906\u001b[39m     \u001b[43mresp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[38;5;66;03m# register connection\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hab/lib/python3.11/site-packages/aiohttp/client_reqrep.py:636\u001b[39m, in \u001b[36mClientResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    634\u001b[39m     \u001b[38;5;28mself\u001b[39m.release()\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[32m    637\u001b[39m     \u001b[38;5;28mself\u001b[39m.request_info,\n\u001b[32m    638\u001b[39m     \u001b[38;5;28mself\u001b[39m.history,\n\u001b[32m    639\u001b[39m     status=\u001b[38;5;28mself\u001b[39m.status,\n\u001b[32m    640\u001b[39m     message=\u001b[38;5;28mself\u001b[39m.reason,\n\u001b[32m    641\u001b[39m     headers=\u001b[38;5;28mself\u001b[39m.headers,\n\u001b[32m    642\u001b[39m )\n",
      "\u001b[31mClientResponseError\u001b[39m: 400, message='', url='https://opendap.earthdata.nasa.gov/thredds/ncss/ornldaac/2129/daymet_v4_daily_na_dayl_2024.nc?var=dayl&north=42.902360&west=-83.475523&east=-78.857244&south=41.383397&disableProjSubset=on&horizStride=1&time_start=2024-01-01T12:00:00Z&time_end=2024-12-30T12:00:00Z&timeStride=1&addLatLon=true&accept=netcdf'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mServiceError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m lakeid = row[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 取 Daymet 栅格（daily）\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m ds = \u001b[43mdaymet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_bygeom\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVARS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# time_scale 默认 daily\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 如你的 GPKG 不是4326，可加 loc_crs=你的EPSG\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 按空间维度(y,x)做湖面平均；skipna 处理缺测\u001b[39;00m\n\u001b[32m     17\u001b[39m ds_mean = ds[VARS].mean(dim=[\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m], skipna=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hab/lib/python3.11/site-packages/pydaymet/pydaymet.py:463\u001b[39m, in \u001b[36mget_bygeom\u001b[39m\u001b[34m(geometry, dates, crs, variables, region, time_scale, pet, pet_params, snow, snow_params, conn_timeout, validate_filesize)\u001b[39m\n\u001b[32m    453\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InputRangeError(\u001b[33m\"\u001b[39m\u001b[33mgeometry\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwithin \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdaymet.region_bbox[region].bounds\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    455\u001b[39m urls = _gridded_urls(\n\u001b[32m    456\u001b[39m     daymet.time_codes[time_scale],\n\u001b[32m    457\u001b[39m     _geometry.bounds,\n\u001b[32m   (...)\u001b[39m\u001b[32m    460\u001b[39m     dates_itr,\n\u001b[32m    461\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m clm_files = \u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43murls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_filesize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    465\u001b[39m     \u001b[38;5;66;03m# open_mfdataset can run into too many open files error so we use merge\u001b[39;00m\n\u001b[32m    466\u001b[39m     \u001b[38;5;66;03m# https://docs.xarray.dev/en/stable/user-guide/io.html#reading-multi-file-datasets\u001b[39;00m\n\u001b[32m    467\u001b[39m     clm = xr.merge(_open_dataset(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m clm_files)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hab/lib/python3.11/site-packages/pydaymet/_utils.py:265\u001b[39m, in \u001b[36mdownload_files\u001b[39m\u001b[34m(url_list, f_ext, validate_filesize, timeout)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validate_filesize \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(f.exists() \u001b[38;5;129;01mand\u001b[39;00m f.stat().st_size > \u001b[32m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m file_list):\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file_list\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m \u001b[43mterry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m file_list\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hab/lib/python3.11/site-packages/tiny_retriever/tiny_retriever.py:220\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(urls, file_paths, chunk_size, limit_per_host, timeout, raise_status)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m parent_dir \u001b[38;5;129;01min\u001b[39;00m {f.parent \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m file_paths}:\n\u001b[32m    218\u001b[39m     parent_dir.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m \u001b[43m_run_in_event_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_download_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43murls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_per_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_status\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hab/lib/python3.11/site-packages/tiny_retriever/tiny_retriever.py:126\u001b[39m, in \u001b[36m_run_in_event_loop\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run a coroutine in the dedicated asyncio event loop.\"\"\"\u001b[39;00m\n\u001b[32m    125\u001b[39m handler = _get_loop_handler()\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_coroutine_threadsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoro\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hab/lib/python3.11/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hab/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hab/lib/python3.11/site-packages/tiny_retriever/tiny_retriever.py:174\u001b[39m, in \u001b[36m_download_session\u001b[39m\u001b[34m(urls, files, limit_per_host, timeout, chunk_size, raise_status)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m ClientSession(\n\u001b[32m    162\u001b[39m     connector=TCPConnector(limit_per_host=limit_per_host, limit=MAX_CONCURRENT_CALLS),\n\u001b[32m    163\u001b[39m     loop=_get_loop_handler().loop,\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m     raise_for_status=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    167\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m    168\u001b[39m     tasks = [\n\u001b[32m    169\u001b[39m         asyncio.create_task(\n\u001b[32m    170\u001b[39m             _stream_file(session, url, filepath, chunk_size, raise_status, timeout)\n\u001b[32m    171\u001b[39m         )\n\u001b[32m    172\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m url, filepath \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(urls, files)\n\u001b[32m    173\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hab/lib/python3.11/site-packages/tiny_retriever/tiny_retriever.py:149\u001b[39m, in \u001b[36m_stream_file\u001b[39m\u001b[34m(session, url, filepath, chunk_size, raise_status, timeout)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ClientResponseError, ClientConnectorDNSError, \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raise_status:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ServiceError(\u001b[38;5;28mstr\u001b[39m(ex), \u001b[38;5;28mstr\u001b[39m(url)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n",
      "\u001b[31mServiceError\u001b[39m: Service returned the following error message:\nURL: https://thredds.daac.ornl.gov/thredds/ncss/ornldaac/2129/daymet_v4_daily_na_vp_2024.nc?var=vp&north=42.902360&west=-83.475523&east=-78.857244&south=41.383397&disableProjSubset=on&horizStride=1&time_start=2024-01-01T12%3A00%3A00Z&time_end=2024-12-30T12%3A00%3A00Z&timeStride=1&addLatLon=true&accept=netcdf\nERROR: 400, message='', url='https://opendap.earthdata.nasa.gov/thredds/ncss/ornldaac/2129/daymet_v4_daily_na_dayl_2024.nc?var=dayl&north=42.902360&west=-83.475523&east=-78.857244&south=41.383397&disableProjSubset=on&horizStride=1&time_start=2024-01-01T12:00:00Z&time_end=2024-12-30T12:00:00Z&timeStride=1&addLatLon=true&accept=netcdf'\n"
     ]
    }
   ],
   "source": [
    "out_dir = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/Daymet/glakes_nc\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dfs_daily = []   # 存各湖“日尺度湖面平均”的表\n",
    "\n",
    "for idx, row in lakes.iterrows():\n",
    "    geom   = row.geometry\n",
    "    lakeid = row[\"name\"]\n",
    "\n",
    "    # 取 Daymet 栅格（daily）\n",
    "    ds = daymet.get_bygeom(\n",
    "        geom, DATES, variables=VARS,  # time_scale 默认 daily\n",
    "        # 如你的 GPKG 不是4326，可加 loc_crs=你的EPSG\n",
    "    )\n",
    "\n",
    "    # 按空间维度(y,x)做湖面平均；skipna 处理缺测\n",
    "    ds_mean = ds[VARS].mean(dim=[\"y\",\"x\"], skipna=True)\n",
    "\n",
    "    # 存 netCDF 以便复用\n",
    "    ds.to_netcdf(out_dir / f\"daymet_{lakeid}_daily.nc\")\n",
    "\n",
    "    # 转成 DataFrame（带时间索引）\n",
    "    df = ds_mean.to_dataframe().reset_index()\n",
    "    df = df.rename(columns={\"time\": \"date\"})\n",
    "    df[\"lake_id\"] = lakeid\n",
    "    dfs_daily.append(df)\n",
    "\n",
    "daymet_daily = pd.concat(dfs_daily, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef7955",
   "metadata": {},
   "source": [
    "与 ILW 时间步对齐：周均 / 28 天滑动均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先确保日期是 datetime\n",
    "daymet_daily[\"date\"] = pd.to_datetime(daymet_daily[\"date\"])\n",
    "\n",
    "# --- 周聚合（自然周，对齐周一/周日可改 label/closed） ---\n",
    "wk = (daymet_daily\n",
    "      .set_index(\"date\")\n",
    "      .groupby(\"lake_id\")\n",
    "      [VARS].resample(\"7D\").mean()\n",
    "      .reset_index()\n",
    "     )\n",
    "\n",
    "# --- 28天移动平均（滑窗，不跳步；末端可能因窗口不满而NA） ---\n",
    "daymet_daily = daymet_daily.sort_values([\"lake_id\",\"date\"])\n",
    "roll28 = (daymet_daily\n",
    "          .groupby(\"lake_id\")[VARS]\n",
    "          .rolling(\"28D\", on=daymet_daily[\"date\"])\n",
    "          .mean()\n",
    "          .reset_index()\n",
    "         )\n",
    "roll28 = roll28.rename(columns={\"level_1\":\"date\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e5fdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1453463249.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mcurl -L -n -c ~/.urs_cookies -b ~/.urs_cookies -f \\\u001b[39m\n                  ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd311e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
