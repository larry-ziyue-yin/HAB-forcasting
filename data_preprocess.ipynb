{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73043a68",
   "metadata": {},
   "source": [
    "# Forecasting Freshwater Algal Bloom Levels Using Multisource Climate and Water-Quality Data\n",
    "\n",
    "*This is the course project of **STATS 402: Interdisciplinary Data Analysis**.*\n",
    "\n",
    "**Name:** Ziyue Yin\n",
    "\n",
    "**NetID:** zy166"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac208a0",
   "metadata": {},
   "source": [
    "## Dataset: HydroLAKES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ef2d1",
   "metadata": {},
   "source": [
    "Lake polygons (including all attributes) in shapefileformat: https://www.hydrosheds.org/products/hydrolakes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e9d58b",
   "metadata": {},
   "source": [
    "## Dataset: NASA OceanColor Inland Waters (ILW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920eaa6",
   "metadata": {},
   "source": [
    "S3Merged-ILW data: https://oceandata.sci.gsfc.nasa.gov/directdataaccess/Level-3%20Mapped/Merged-S3-ILW/.\n",
    "\n",
    "S3B-ILW data: https://oceandata.sci.gsfc.nasa.gov/directdataaccess/Level-3%20Mapped/S3B-ILW/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0419fb8",
   "metadata": {},
   "source": [
    "After downloading the datasets, the structure should be shown as follows:\n",
    "\n",
    "```\n",
    "datasets/\n",
    " ├── ILW/\n",
    " │    ├── S3B/2024/CONUS_MO/\n",
    " │    │      ├── S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    │      ├── S3B_OLCI_EFRNT.20240201_20240229.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    ├── Merged/2024/CONUS_DAY/\n",
    " │    │      ├── S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    │      ├── S3M_OLCI_EFRNT.20240102.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    │      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e70cade",
   "metadata": {},
   "source": [
    "### Data Structure Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e84bc5",
   "metadata": {},
   "source": [
    "First of all, let's glance at the monthly dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3545ae05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenMappingWarningOnValuesAccess({'y': 15138, 'x': 26328, 'rgb': 3, 'eightbitcolor': 256})\n",
      "['rhos_400', 'rhos_412', 'rhos_443', 'rhos_490', 'rhos_510', 'rhos_560', 'rhos_620', 'rhos_665', 'rhos_674', 'rhos_681', 'rhos_709', 'rhos_754', 'rhos_865', 'rhos_884', 'CI_cyano', 'palette']\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "p = \"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/S3B/2024/CONUS_MO/S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\"\n",
    "\n",
    "ds = xr.open_dataset(p, engine=\"netcdf4\", chunks=\"auto\")\n",
    "print(ds.dims)\n",
    "print(list(ds.data_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b94fb",
   "metadata": {},
   "source": [
    "And also, the daily dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a063b2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenMappingWarningOnValuesAccess({'y': 15138, 'x': 26328, 'rgb': 3, 'eightbitcolor': 256})\n",
      "['rhos_400', 'rhos_412', 'rhos_443', 'rhos_490', 'rhos_510', 'rhos_560', 'rhos_620', 'rhos_665', 'rhos_674', 'rhos_681', 'rhos_709', 'rhos_754', 'rhos_865', 'rhos_884', 'CI_cyano', 'palette']\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "p = \"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\"\n",
    "\n",
    "ds = xr.open_dataset(p, engine=\"netcdf4\", chunks=\"auto\")\n",
    "print(ds.dims)\n",
    "print(list(ds.data_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627b349",
   "metadata": {},
   "source": [
    "### Target Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edbba54",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d9caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "\n",
    "def infer_time_label(nc_path, ds, product=\"monthly\"):\n",
    "    \"\"\"\n",
    "    Return a pandas.Timestamp, try to infer from ds or filename.\n",
    "    product: 'monthly' or 'daily'\n",
    "    \"\"\"\n",
    "    # 1) Directly have time coordinate/variable\n",
    "    if \"time\" in ds.coords or \"time\" in ds.variables:\n",
    "        try:\n",
    "            tt = pd.to_datetime(ds[\"time\"].values)\n",
    "            tt = np.array(tt).reshape(-1)[0]\n",
    "            return pd.to_datetime(tt)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2) Global attributes (common in L3M data)\n",
    "    start = ds.attrs.get(\"time_coverage_start\") or ds.attrs.get(\"start_time\")\n",
    "    end   = ds.attrs.get(\"time_coverage_end\")   or ds.attrs.get(\"end_time\")\n",
    "    if start and end:\n",
    "        try:\n",
    "            ts = pd.to_datetime(start)\n",
    "            te = pd.to_datetime(end)\n",
    "            if product == \"monthly\":\n",
    "                return ts + (te - ts) / 2\n",
    "            else:\n",
    "                return ts\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 3) Analyze the filename\n",
    "    fn = nc_path.split(\"/\")[-1]\n",
    "    if product == \"monthly\":\n",
    "        # ...YYYYMMDD_YYYYMMDD.L3m.MO...\n",
    "        m = re.search(r\"\\.(\\d{8})_(\\d{8})\\.L3m\\.MO\\.\", fn)\n",
    "        if m:\n",
    "            b, e = m.group(1), m.group(2)\n",
    "            ts = pd.to_datetime(b, format=\"%Y%m%d\")\n",
    "            te = pd.to_datetime(e, format=\"%Y%m%d\")\n",
    "            return ts + (te - ts) / 2\n",
    "    else:\n",
    "        # ...YYYYMMDD.L3m.DAY...\n",
    "        m = re.search(r\"\\.(\\d{8})\\.L3m\\.DAY\\.\", fn)\n",
    "        if m:\n",
    "            return pd.to_datetime(m.group(1), format=\"%Y%m%d\")\n",
    "\n",
    "    raise ValueError(\"Cannot infer time from dataset or filename: \" + fn)\n",
    "\n",
    "def clean_ci(da: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Filter out values out of physical range and remove near-zero values.\n",
    "    \"\"\"\n",
    "    vmin = float(da.attrs.get(\"valid_min\", np.nan))\n",
    "    vmax = float(da.attrs.get(\"valid_max\", np.nan))\n",
    "    if np.isfinite(vmin):\n",
    "        da = da.where(da >= vmin)\n",
    "    if np.isfinite(vmax):\n",
    "        da = da.where(da <= vmax)\n",
    "\n",
    "    thr = max(vmin, 5e-5) if np.isfinite(vmin) else 5e-5\n",
    "    da = da.where(da > thr)\n",
    "\n",
    "    return da\n",
    "\n",
    "def extract_lakes_from_nc(nc_path: str,\n",
    "                          lakes_gdf: gpd.GeoDataFrame,\n",
    "                          lake_id_col: str,\n",
    "                          product: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    nc_path: A single NetCDF file (S3B monthly or S3M daily)\n",
    "    lakes_gdf: A GeoDataFrame containing `lake_id` and `geometry` (EPSG:4326)\n",
    "    product: 'monthly' | 'daily'\n",
    "    Returns: One row per lake (timestamp of the file)\n",
    "    \"\"\"\n",
    "    with xr.open_dataset(nc_path, engine=\"netcdf4\", chunks=\"auto\") as ds:\n",
    "        t  = infer_time_label(nc_path, ds, product=product)\n",
    "        da = set_spatial_dims_safe(ds[\"CI_cyano\"], ds=ds)\n",
    "        da = clean_ci(da).rio.write_crs(4326)\n",
    "\n",
    "        rows = []\n",
    "        for _, row in lakes_gdf.iterrows():\n",
    "            lid  = row[lake_id_col]\n",
    "            geom = [row.geometry]\n",
    "            try:\n",
    "                clipped = da.rio.clip(geom, lakes_gdf.crs, drop=True)\n",
    "                arr     = clipped.data  # dask/np 数组\n",
    "                arr     = np.asarray(arr)  # 触发 dask 计算\n",
    "                mask    = np.isfinite(arr)\n",
    "                n_valid = int(mask.sum())\n",
    "\n",
    "                if n_valid == 0:\n",
    "                    mean_val = np.nan\n",
    "                    p90      = np.nan\n",
    "                else:\n",
    "                    vals = arr[mask].ravel()\n",
    "                    mean_val = float(np.nanmean(vals))\n",
    "                    p90      = float(np.nanquantile(vals, 0.9))\n",
    "            except Exception:\n",
    "                mean_val, p90, n_valid = np.nan, np.nan, 0\n",
    "\n",
    "            rows.append({\n",
    "                \"lake_id\": lid, \"time\": pd.to_datetime(t), \"product\": product,\n",
    "                \"CI_mean\": mean_val, \"CI_p90\": p90, \"n_valid\": n_valid,\n",
    "                \"src\": Path(nc_path).name,\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def set_spatial_dims_safe(da: xr.DataArray, ds: xr.Dataset | None = None) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Make `da` geospatially aware for rioxarray:\n",
    "    - Prefer real dimension names present on `da` (lon/lat or x/y or variants);\n",
    "    - Only pass EXISTING dimension names to `rio.set_spatial_dims`;\n",
    "    - If dataset carries 1D lon/lat arrays matching x/y lengths, bind them as coords;\n",
    "    - Finally write CRS=EPSG:4326.\n",
    "\n",
    "    NOTE: If lon/lat are 2D (curvilinear), we DO NOT pass them as dims; we keep x/y dims.\n",
    "    \"\"\"\n",
    "    dims = list(da.dims)\n",
    "\n",
    "    # normalize common variants\n",
    "    def _has(*cands):\n",
    "        return any(c in dims for c in cands)\n",
    "\n",
    "    # Case A: dims already lon/lat\n",
    "    if \"lon\" in dims and \"lat\" in dims:\n",
    "        out = da.rio.write_crs(4326)\n",
    "        out = out.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=False)\n",
    "    elif \"longitude\" in dims and \"latitude\" in dims:\n",
    "        out = da.rename({\"longitude\": \"lon\", \"latitude\": \"lat\"})\n",
    "        out = out.rio.write_crs(4326)\n",
    "        out = out.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=False)\n",
    "\n",
    "    # Case B: dims are x/y (any case)\n",
    "    elif (_has(\"x\") and _has(\"y\")) or (_has(\"X\") and _has(\"Y\")):\n",
    "        # rename upper-case to lower-case to please rioxarray\n",
    "        rename_map = {}\n",
    "        if \"X\" in dims: rename_map[\"X\"] = \"x\"\n",
    "        if \"Y\" in dims: rename_map[\"Y\"] = \"y\"\n",
    "        out = da.rename(rename_map) if rename_map else da\n",
    "        out = out.rio.write_crs(4326)\n",
    "        out = out.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "\n",
    "        # try binding 1D lon/lat coords from ds if lengths match\n",
    "        if ds is not None:\n",
    "            try:\n",
    "                lon1d = None\n",
    "                lat1d = None\n",
    "                for lon_name in (\"lon\",\"longitude\"):\n",
    "                    if lon_name in ds.variables and ds[lon_name].ndim == 1 and ds[lon_name].sizes[ds[lon_name].dims[0]] == out.sizes[\"x\"]:\n",
    "                        lon1d = np.asarray(ds[lon_name].values)\n",
    "                        break\n",
    "                for lat_name in (\"lat\",\"latitude\"):\n",
    "                    if lat_name in ds.variables and ds[lat_name].ndim == 1 and ds[lat_name].sizes[ds[lat_name].dims[0]] == out.sizes[\"y\"]:\n",
    "                        lat1d = np.asarray(ds[lat_name].values)\n",
    "                        break\n",
    "                if lon1d is not None and lat1d is not None:\n",
    "                    out = out.assign_coords(x=lon1d, y=lat1d)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Case C: unknown names → rename the last two dimensions to y/x\n",
    "    elif len(dims) >= 2:\n",
    "        ydim, xdim = dims[-2], dims[-1]\n",
    "        out = da.rename({xdim: \"x\", ydim: \"y\"}).rio.write_crs(4326)\n",
    "        out = out.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot determine spatial dims for {da.name!r}; dims={dims}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "def looks_like_hdf5(path: Path) -> bool:\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            sig = f.read(8)\n",
    "        return sig.startswith(b\"\\x89HDF\") or sig.startswith(b\"CDF\")\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _extract_one_with_h5netcdf(nc_path: Path,\n",
    "                               lakes_gdf: gpd.GeoDataFrame,\n",
    "                               lake_id_col: str,\n",
    "                               product: str = \"daily\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    兜底方案：用 h5netcdf 打开并在本函数内完成裁剪与统计。\n",
    "    只有当 extract_lakes_from_nc 失败时才会被调用。\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    # 注意：phony_dims=\"access\" 让 h5netcdf 创建可访问维度名\n",
    "    with xr.open_dataset(nc_path, engine=\"h5netcdf\", chunks=\"auto\", phony_dims=\"access\") as ds:\n",
    "        t  = infer_time_label(str(nc_path), ds, product=product)\n",
    "        da = set_spatial_dims_safe(ds[\"CI_cyano\"], ds=ds)\n",
    "        da = clean_ci(da).rio.write_crs(4326)\n",
    "\n",
    "        for _, r in lakes_gdf.iterrows():\n",
    "            lid  = r[lake_id_col]\n",
    "            geom = [r.geometry]\n",
    "            try:\n",
    "                clipped = da.rio.clip(geom, lakes_gdf.crs, drop=True)\n",
    "                arr     = np.asarray(clipped.data)\n",
    "                mask    = np.isfinite(arr)\n",
    "                n_valid = int(mask.sum())\n",
    "                if n_valid == 0:\n",
    "                    mean_val, p90 = np.nan, np.nan\n",
    "                else:\n",
    "                    vals = arr[mask].ravel()\n",
    "                    mean_val = float(np.nanmean(vals))\n",
    "                    p90      = float(np.nanquantile(vals, 0.9))\n",
    "            except Exception:\n",
    "                mean_val, p90, n_valid = np.nan, np.nan, 0\n",
    "\n",
    "            rows.append({\n",
    "                \"lake_id\": lid,\n",
    "                \"date\":    pd.to_datetime(t),\n",
    "                \"product\": product,\n",
    "                \"CI_mean\": mean_val,\n",
    "                \"CI_p90\":  p90,\n",
    "                \"n_valid\": n_valid,\n",
    "                \"src\":     nc_path.name,\n",
    "                \"engine\":  \"h5netcdf\",\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def try_open_xarray(fp: Path):\n",
    "    \"\"\"Try netcdf4 → h5netcdf → h5py，return (ds or None, engine_used)。\"\"\"\n",
    "    try:\n",
    "        ds = xr.open_dataset(fp, engine=\"netcdf4\", chunks=\"auto\")\n",
    "        _ = ds.dims\n",
    "        return ds, \"netcdf4\"\n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            ds = xr.open_dataset(fp, engine=\"h5netcdf\", chunks=\"auto\", phony_dims=\"access\")\n",
    "            _ = ds.dims\n",
    "            return ds, \"h5netcdf\"\n",
    "        except Exception as e2:\n",
    "            try:\n",
    "                with h5py.File(fp, \"r\") as f:\n",
    "                    pass\n",
    "                print(f\"[WARN] {fp.name} readable by h5py but not by xarray engines (netCDF-4 layout issue?)\")\n",
    "            except Exception as e3:\n",
    "                print(f\"[WARN] {fp.name} not readable even by h5py: {e3}\")\n",
    "            print(f\"[SKIP] {fp.name} → netcdf4:{e1} | h5netcdf:{e2}\")\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4665a61",
   "metadata": {},
   "source": [
    "#### Scale 1: In general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f52fe",
   "metadata": {},
   "source": [
    "##### Monthly\n",
    "\n",
    "Here, we use the **S3B Monthly** data. One month per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f98a86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>CI_mean</th>\n",
       "      <th>CI_p90</th>\n",
       "      <th>n_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-16 16:53:28.500000+00:00</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>2030281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-15 17:12:11.500000+00:00</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>2204191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-16 16:44:09+00:00</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>1810493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-16 04:52:02+00:00</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>2151149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-05-16 17:01:29+00:00</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>2586974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              time   CI_mean    CI_p90  n_valid\n",
       "0 2024-01-16 16:53:28.500000+00:00  0.000591  0.000472  2030281\n",
       "1 2024-02-15 17:12:11.500000+00:00  0.000539  0.000351  2204191\n",
       "2        2024-03-16 16:44:09+00:00  0.000492  0.000166  1810493\n",
       "3        2024-04-16 04:52:02+00:00  0.000496  0.000540  2151149\n",
       "4        2024-05-16 17:01:29+00:00  0.000500  0.000836  2586974"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob, numpy as np, pandas as pd, xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "monthly_dir = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/S3B/2024/CONUS_MO\")\n",
    "out_csv = monthly_dir/\"ci_cyano_monthly_mean.csv\"\n",
    "\n",
    "rows = []\n",
    "for fp in sorted(monthly_dir.glob(\"S3B_OLCI_EFRNT.*.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\")):\n",
    "    with xr.open_dataset(fp, engine=\"netcdf4\", chunks=\"auto\") as ds:\n",
    "        da = clean_ci(ds[\"CI_cyano\"])\n",
    "        arr = np.asarray(da.data)\n",
    "        mask = np.isfinite(arr)\n",
    "        m   = float(np.nanmean(arr[mask])) if mask.any() else np.nan\n",
    "        p90 = float(np.nanquantile(arr[mask], 0.9))    if mask.any() else np.nan\n",
    "        t   = infer_time_label(str(fp), ds, product=\"monthly\")\n",
    "        rows.append({\"time\": pd.to_datetime(t), \"CI_mean\": m, \"CI_p90\": p90,\n",
    "                     \"n_valid\": int(mask.sum())})\n",
    "\n",
    "df_mo = pd.DataFrame(rows).sort_values(\"time\").reset_index(drop=True)\n",
    "df_mo.to_csv(out_csv, index=False)\n",
    "df_mo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b93b8d",
   "metadata": {},
   "source": [
    "##### Daily\n",
    "\n",
    "Here, we use the **S3M Daily** data. One day per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1970169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] S3M_OLCI_EFRNT.20240211.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc not readable even by h5py: Unable to synchronously open file (truncated file: eof = 323158016, sblock->base_addr = 0, stored_eof = 763662772)\n",
      "[SKIP] S3M_OLCI_EFRNT.20240211.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc → netcdf4:[Errno -101] NetCDF: HDF error: '/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20240211.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc' | h5netcdf:Unable to synchronously open file (truncated file: eof = 323158016, sblock->base_addr = 0, stored_eof = 763662772)\n",
      "[WARN] S3M_OLCI_EFRNT.20241016.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc not readable even by h5py: Unable to synchronously open file (truncated file: eof = 411697152, sblock->base_addr = 0, stored_eof = 978029795)\n",
      "[SKIP] S3M_OLCI_EFRNT.20241016.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc → netcdf4:[Errno -101] NetCDF: HDF error: '/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20241016.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc' | h5netcdf:Unable to synchronously open file (truncated file: eof = 411697152, sblock->base_addr = 0, stored_eof = 978029795)\n",
      "[OK] saved → /dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/ci_cyano_daily_mean.csv (rows=361)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>CI_mean</th>\n",
       "      <th>CI_p90</th>\n",
       "      <th>n_valid</th>\n",
       "      <th>src</th>\n",
       "      <th>engine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 13:52:31+00:00</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1024210</td>\n",
       "      <td>S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.a...</td>\n",
       "      <td>netcdf4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02 13:36:49+00:00</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1047655</td>\n",
       "      <td>S3M_OLCI_EFRNT.20240102.L3m.DAY.ILW_CONUS.V5.a...</td>\n",
       "      <td>netcdf4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03 14:02:06+00:00</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>865219</td>\n",
       "      <td>S3M_OLCI_EFRNT.20240103.L3m.DAY.ILW_CONUS.V5.a...</td>\n",
       "      <td>netcdf4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04 13:40:26+00:00</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1144789</td>\n",
       "      <td>S3M_OLCI_EFRNT.20240104.L3m.DAY.ILW_CONUS.V5.a...</td>\n",
       "      <td>netcdf4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-05 13:48:41+00:00</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1152641</td>\n",
       "      <td>S3M_OLCI_EFRNT.20240105.L3m.DAY.ILW_CONUS.V5.a...</td>\n",
       "      <td>netcdf4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date   CI_mean   CI_p90  n_valid  \\\n",
       "0 2024-01-01 13:52:31+00:00  0.000432  0.00005  1024210   \n",
       "1 2024-01-02 13:36:49+00:00  0.000274  0.00005  1047655   \n",
       "2 2024-01-03 14:02:06+00:00  0.000368  0.00005   865219   \n",
       "3 2024-01-04 13:40:26+00:00  0.000230  0.00005  1144789   \n",
       "4 2024-01-05 13:48:41+00:00  0.000273  0.00005  1152641   \n",
       "\n",
       "                                                 src   engine  \n",
       "0  S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  \n",
       "1  S3M_OLCI_EFRNT.20240102.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  \n",
       "2  S3M_OLCI_EFRNT.20240103.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  \n",
       "3  S3M_OLCI_EFRNT.20240104.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  \n",
       "4  S3M_OLCI_EFRNT.20240105.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import h5py\n",
    "import geopandas as gpd\n",
    "\n",
    "daily_dir = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY\")\n",
    "out_csv = daily_dir / \"ci_cyano_daily_mean.csv\"\n",
    "\n",
    "rows = []\n",
    "for fp in sorted(daily_dir.glob(\"S3M_OLCI_EFRNT.*.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\")):\n",
    "    if not looks_like_hdf5(fp):\n",
    "        print(f\"[WARN] Skip (not HDF5 header): {fp.name}\")\n",
    "        continue\n",
    "\n",
    "    ds, eng = try_open_xarray(fp)\n",
    "    if ds is None:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        da = clean_ci(ds[\"CI_cyano\"])\n",
    "        arr = np.asarray(da.data)\n",
    "        mask = np.isfinite(arr)\n",
    "        m   = float(np.nanmean(arr[mask])) if mask.any() else np.nan\n",
    "        p90 = float(np.nanquantile(arr[mask], 0.9)) if mask.any() else np.nan\n",
    "\n",
    "        t = infer_time_label(str(fp), ds, product=\"daily\")\n",
    "\n",
    "        rows.append({\n",
    "            \"date\":   pd.to_datetime(t),\n",
    "            \"CI_mean\": m,\n",
    "            \"CI_p90\":  p90,\n",
    "            \"n_valid\": int(mask.sum()),\n",
    "            \"src\":     fp.name,\n",
    "            \"engine\":  eng,\n",
    "        })\n",
    "    finally:\n",
    "        ds.close()\n",
    "\n",
    "df_day = pd.DataFrame(rows).sort_values(\"date\").reset_index(drop=True)\n",
    "df_day.to_csv(out_csv, index=False)\n",
    "print(f\"[OK] saved → {out_csv} (rows={len(df_day)})\")\n",
    "df_day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4fc325",
   "metadata": {},
   "source": [
    "#### Scale 2: Five Great Lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b2c1f",
   "metadata": {},
   "source": [
    "First of all, we get the five Great Lakes out explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f6808a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg features: 5\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "\n",
    "src = \"/dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes.gpkg\"\n",
    "gdf = gpd.read_file(src)\n",
    "\n",
    "keep_names = [\"Superior\",\"Michigan\",\"Huron\",\"Erie\",\"Ontario\"]\n",
    "gdf = gpd.read_file(src)\n",
    "gdf5 = gdf[gdf[\"Lake_name\"].str.fullmatch(\"|\".join(keep_names), case=False)].copy()\n",
    "gdf5 = gdf5.dissolve(by=\"Lake_name\", as_index=False).rename(columns={\"Lake_name\":\"lake_name\"})\n",
    "\n",
    "order = [\"Erie\",\"Huron\",\"Michigan\",\"Ontario\",\"Superior\"]\n",
    "gdf5 = gdf5.set_index(\"lake_name\").loc[order].reset_index()\n",
    "\n",
    "gdf5m = gdf5.to_crs(5070)\n",
    "gdf5m[\"geometry\"] = gdf5m.buffer(-300)\n",
    "gdf5m = gdf5m[~gdf5m.geometry.is_empty].copy()\n",
    "gdf5  = gdf5m.to_crs(4326).reset_index(drop=True)\n",
    "\n",
    "gdf5[\"lake_id\"] = [f\"GL-{i+1}\" for i in range(len(gdf5))]\n",
    "\n",
    "# Buffer the shoreline by 300 m, first to equidistant projection, then buffer, then back to 4326\n",
    "gdf5m = gdf5.to_crs(5070)\n",
    "gdf5m[\"geometry\"] = gdf5m.buffer(-300)\n",
    "gdf5 = gdf5m.to_crs(4326)\n",
    "\n",
    "out = Path(src).with_name(\"lakes_greatlakes_5poly.gpkg\")\n",
    "gdf5.to_file(out, driver=\"GPKG\")\n",
    "print(\"Saved:\", out, \"features:\", len(gdf5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843c8b2d",
   "metadata": {},
   "source": [
    "##### Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb3e1315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[monthly] saved → /dkucc/home/zy166/HAB-forecasting/datasets/processed/lake_ci_monthly.parquet  (60 rows)\n"
     ]
    }
   ],
   "source": [
    "def run_monthly(monthly_dir: str,\n",
    "                lakes_fp: str,\n",
    "                lake_id_col: str,\n",
    "                out_parquet: str):\n",
    "    \"\"\"\n",
    "    monthly_dir: Directory containing files like S3B_OLCI_EFRNT.*.L3m.MO.ILW_CONUS...nc\n",
    "    lakes_fp:    Lake boundaries (gpkg/shp, must be EPSG:4326)\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(lakes_fp)\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"The lake file is missing CRS, please ensure it is EPSG:4326\")\n",
    "    gdf = gdf.to_crs(4326)[[lake_id_col, \"geometry\"]].dropna()\n",
    "\n",
    "    out_rows = []\n",
    "    for fp in sorted(Path(monthly_dir).glob(\"S3B_OLCI_EFRNT.*.L3m.MO.*.nc\")):\n",
    "        df_one = extract_lakes_from_nc(str(fp), gdf, lake_id_col, product=\"monthly\")\n",
    "        out_rows.append(df_one)\n",
    "\n",
    "    if not out_rows:\n",
    "        print(\"No monthly files found.\")\n",
    "        return\n",
    "\n",
    "    df_all = pd.concat(out_rows, ignore_index=True)\n",
    "    Path(out_parquet).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_all.to_parquet(out_parquet, index=False)\n",
    "    print(f\"[monthly] saved → {out_parquet}  ({len(df_all)} rows)\")\n",
    "\n",
    "run_monthly(\n",
    "    monthly_dir=\"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/S3B/2024/CONUS_MO\",\n",
    "    lakes_fp=\"/dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\",\n",
    "    lake_id_col=\"lake_id\",\n",
    "    out_parquet=\"/dkucc/home/zy166/HAB-forecasting/datasets/processed/lake_ci_monthly.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c23b3",
   "metadata": {},
   "source": [
    "##### Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea7c2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] netcdf4 failed on S3M_OLCI_EFRNT.20240211.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc: [Errno -101] NetCDF: HDF error: '/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20240211.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc'\n",
      "[SKIP] S3M_OLCI_EFRNT.20240211.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc: h5netcdf fallback failed → Unable to synchronously open file (truncated file: eof = 323158016, sblock->base_addr = 0, stored_eof = 763662772)\n",
      "[WARN] netcdf4 failed on S3M_OLCI_EFRNT.20241016.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc: [Errno -101] NetCDF: HDF error: '/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20241016.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc'\n",
      "[SKIP] S3M_OLCI_EFRNT.20241016.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc: h5netcdf fallback failed → Unable to synchronously open file (truncated file: eof = 411697152, sblock->base_addr = 0, stored_eof = 978029795)\n",
      "[daily] saved → /dkucc/home/zy166/HAB-forecasting/datasets/processed/lake_ci_daily.parquet  (rows=1805, files=363)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "\n",
    "def run_daily(daily_dir: str,\n",
    "              lakes_fp: str,\n",
    "              lake_id_col: str,\n",
    "              out_parquet: str):\n",
    "    \"\"\"\n",
    "    daily_dir: Directory containing files like S3M_OLCI_EFRNT.*.L3m.DAY.ILW_CONUS...nc\n",
    "    \"\"\"\n",
    "    daily_dir = Path(daily_dir)\n",
    "    gdf = gpd.read_file(lakes_fp)\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"The lake file is missing CRS, please ensure it is EPSG:4326\")\n",
    "    gdf = gdf.to_crs(4326)[[lake_id_col, \"geometry\"]].dropna()\n",
    "\n",
    "    out_rows = []\n",
    "    files = sorted(daily_dir.glob(\"S3M_OLCI_EFRNT.*.L3m.DAY.*.nc\"))\n",
    "    if not files:\n",
    "        print(f\"[WARN] No daily files found under: {daily_dir}\")\n",
    "        return\n",
    "\n",
    "    for fp in files:\n",
    "        # 1) 文件头快速校验\n",
    "        if not looks_like_hdf5(fp):\n",
    "            print(f\"[WARN] Skip (not HDF5/NetCDF header): {fp.name}\")\n",
    "            continue\n",
    "\n",
    "        # 2) 首选：用你已有的函数（netcdf4 引擎打开）\n",
    "        try:\n",
    "            df_one = extract_lakes_from_nc(str(fp), gdf, lake_id_col, product=\"daily\")\n",
    "            # 统一列名为 date\n",
    "            if \"time\" in df_one.columns and \"date\" not in df_one.columns:\n",
    "                df_one = df_one.rename(columns={\"time\": \"date\"})\n",
    "            df_one[\"src\"] = fp.name\n",
    "            df_one[\"engine\"] = \"netcdf4\"\n",
    "            out_rows.append(df_one)\n",
    "            continue\n",
    "        except Exception as e1:\n",
    "            print(f\"[WARN] netcdf4 failed on {fp.name}: {e1}\")\n",
    "\n",
    "        # 3) 兜底：h5netcdf 内联提取（若可用）\n",
    "        try:\n",
    "            df_one = _extract_one_with_h5netcdf(fp, gdf, lake_id_col, product=\"daily\")\n",
    "            out_rows.append(df_one)\n",
    "        except Exception as e2:\n",
    "            print(f\"[SKIP] {fp.name}: h5netcdf fallback failed → {e2}\")\n",
    "\n",
    "    if not out_rows:\n",
    "        print(\"No valid daily rows produced.\")\n",
    "        return\n",
    "\n",
    "    df_all = pd.concat(out_rows, ignore_index=True)\n",
    "\n",
    "    # 基本整理：日期排序、列顺序、小型诊断\n",
    "    keep_cols = [\"lake_id\", \"date\", \"product\", \"CI_mean\", \"CI_p90\", \"n_valid\", \"src\", \"engine\"]\n",
    "    for c in keep_cols:\n",
    "        if c not in df_all.columns:\n",
    "            df_all[c] = np.nan if c not in (\"lake_id\",\"product\",\"src\",\"engine\") else None\n",
    "    df_all = df_all[keep_cols].sort_values([\"lake_id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    Path(out_parquet).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_all.to_parquet(out_parquet, index=False)\n",
    "    print(f\"[daily] saved → {out_parquet}  (rows={len(df_all)}, files={len(files)})\")\n",
    "\n",
    "run_daily(\n",
    "    daily_dir=\"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY\",\n",
    "    lakes_fp=\"/dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\",\n",
    "    lake_id_col=\"lake_id\",\n",
    "    out_parquet=\"/dkucc/home/zy166/HAB-forecasting/datasets/processed/lake_ci_daily.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25773dfe",
   "metadata": {},
   "source": [
    "### Data Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a47a60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['lake_id', 'product', 'CI_mean', 'CI_p90', 'n_valid', 'src', 'date', 'area_m2', 'expected_pixels_geom', 'lake_name', 'empiric_n_valid_max', 'pct_valid_geom', 'pct_valid_emp', 'qc_low_cov_geom', 'qc_low_cov_emp', 'qc_tiny_abs_pix', 'qc_is_valid']\n",
      "Shape: (60, 17)\n",
      "  lake_id  product  CI_mean  CI_p90  n_valid  \\\n",
      "0    GL-1  monthly      NaN     NaN      NaN   \n",
      "1    GL-2  monthly      NaN     NaN      NaN   \n",
      "2    GL-3  monthly      NaN     NaN      NaN   \n",
      "3    GL-4  monthly      NaN     NaN      NaN   \n",
      "4    GL-5  monthly      NaN     NaN      NaN   \n",
      "\n",
      "                                                 src                    date  \\\n",
      "0  S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CO... 2024-01-16 16:53:28.500   \n",
      "1  S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CO... 2024-01-16 16:53:28.500   \n",
      "2  S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CO... 2024-01-16 16:53:28.500   \n",
      "3  S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CO... 2024-01-16 16:53:28.500   \n",
      "4  S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CO... 2024-01-16 16:53:28.500   \n",
      "\n",
      "        area_m2  expected_pixels_geom lake_name  empiric_n_valid_max  \\\n",
      "0  2.488940e+10                276549      Erie                  NaN   \n",
      "1  5.581964e+10                620218     Huron                  NaN   \n",
      "2  5.620858e+10                624540  Michigan                  NaN   \n",
      "3  1.815035e+10                201671   Ontario                  NaN   \n",
      "4  7.928849e+10                880983  Superior                  NaN   \n",
      "\n",
      "   pct_valid_geom  pct_valid_emp  qc_low_cov_geom  qc_low_cov_emp  \\\n",
      "0            <NA>            NaN             <NA>            <NA>   \n",
      "1            <NA>            NaN             <NA>            <NA>   \n",
      "2            <NA>            NaN             <NA>            <NA>   \n",
      "3            <NA>            NaN             <NA>            <NA>   \n",
      "4            <NA>            NaN             <NA>            <NA>   \n",
      "\n",
      "   qc_tiny_abs_pix  qc_is_valid  \n",
      "0                0            1  \n",
      "1                0            1  \n",
      "2                0            1  \n",
      "3                0            1  \n",
      "4                0            1  \n",
      "CI_mean: non-null=0, finite=0, mean=nan\n",
      "CI_p90: non-null=0, finite=0, mean=nan\n",
      "n_valid: non-null=0, finite=0, mean=nan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_parquet(\"/dkucc/home/zy166/HAB-forecasting/datasets/processed/qc/greatlakes_monthly_clean.parquet\")\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "for c in [\"CI_mean\", \"CI_p90\", \"n_valid\"]:\n",
    "    if c in df.columns:\n",
    "        v = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        print(f\"{c}: non-null={v.notna().sum()}, finite={np.isfinite(v).sum()}, mean={v.mean() if np.isfinite(v).any() else np.nan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f4ba19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved regional QC → /dkucc/home/zy166/HAB-forecasting/datasets/processed/qc/conus_daily_clean.csv\n",
      "[OK] Saved regional QC → /dkucc/home/zy166/HAB-forecasting/datasets/processed/qc/conus_monthly_clean.csv\n",
      "[WARN] n_valid is missing or all zeros in lake parquet; set n_valid = NaN so coverage flags become NA.\n",
      "\n",
      "[DIAG] per-lake counts:\n",
      " lake_id  n_rows  n_CI_finite  n_valid_pos  qc_valid\n",
      "   GL-1     361            0            0       361\n",
      "   GL-2     361            0            0       361\n",
      "   GL-3     361            0            0       361\n",
      "   GL-4     361            0            0       361\n",
      "   GL-5     361            0            0       361\n",
      "[OK] Saved lake-level QC → /dkucc/home/zy166/HAB-forecasting/datasets/processed/qc/greatlakes_daily_clean.parquet\n",
      "[OK] Summary → /dkucc/home/zy166/HAB-forecasting/datasets/processed/qc/greatlakes_daily_summary.csv\n",
      "[WARN] n_valid is missing or all zeros in lake parquet; set n_valid = NaN so coverage flags become NA.\n",
      "\n",
      "[DIAG] per-lake counts:\n",
      " lake_id  n_rows  n_CI_finite  n_valid_pos  qc_valid\n",
      "   GL-1      12            0            0        12\n",
      "   GL-2      12            0            0        12\n",
      "   GL-3      12            0            0        12\n",
      "   GL-4      12            0            0        12\n",
      "   GL-5      12            0            0        12\n",
      "[OK] Saved lake-level QC → /dkucc/home/zy166/HAB-forecasting/datasets/processed/qc/greatlakes_monthly_clean.parquet\n",
      "[OK] Summary → /dkucc/home/zy166/HAB-forecasting/datasets/processed/qc/greatlakes_monthly_summary.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "QC & Completeness checks for NASA ILW Cyanobacteria Index (CI_cyano)\n",
    "- Regional (CONUS) daily & monthly time series\n",
    "- Lake-level (Great Lakes) daily & monthly time series\n",
    "\n",
    "Inputs (already prepared by you):\n",
    "1) /dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/ci_cyano_daily_mean.csv\n",
    "2) /dkucc/home/zy166/HAB-forecasting/datasets/ILW/S3B/2024/CONUS_MO/ci_cyano_monthly_mean.csv\n",
    "3) /dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\n",
    "4) /dkucc/home/zy166/HAB-forecasting/datasets/processed/lake_ci_monthly.parquet\n",
    "5) /dkucc/home/zy166/HAB-forecasting/datasets/processed/lake_ci_daily.parquet\n",
    "\n",
    "Outputs:\n",
    "- Cleaned regional daily/monthly CSVs with QC flags\n",
    "- Cleaned lake-level daily/monthly Parquet with QC flags and completeness metrics\n",
    "- Simple summary CSVs per product (row counts, missing rates, clipping, etc.)\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# ----------------------------\n",
    "# Parameters (tune as needed)\n",
    "# ----------------------------\n",
    "# Resolution of ILW L3m grid (meters)\n",
    "PIX_RES_M = 300.0\n",
    "\n",
    "# Minimal valid pixel ratio when judged by geometry (n_valid / expected_pixels)\n",
    "MIN_PCT_VALID_GEOM = 0.10   # drop if coverage < 10%\n",
    "\n",
    "# Minimal absolute valid pixels per lake-time\n",
    "MIN_ABS_PIX = 50            # drop very tiny coverage\n",
    "\n",
    "# Empirical coverage threshold relative to best observed coverage for that lake\n",
    "MIN_PCT_VALID_EMP = 0.10    # drop if n_valid < 10% of lake's max observed n_valid\n",
    "\n",
    "# CI cleaning thresholds\n",
    "NEAR_ZERO_THRESHOLD = 5e-5  # drop near-zero values (already applied in earlier step, kept as doc)\n",
    "CLIP_QUANTILE_LOW  = 0.001  # lower clip for outliers\n",
    "CLIP_QUANTILE_HIGH = 0.999  # upper clip for outliers\n",
    "\n",
    "# Interpolation limits for regional (optional smoothing to fill short gaps)\n",
    "INTERP_LIMIT_DAYS = 3\n",
    "\n",
    "# ----------------------------\n",
    "# Paths\n",
    "# ----------------------------\n",
    "P_CONUS_DAILY   = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/ci_cyano_daily_mean.csv\")\n",
    "P_CONUS_MONTHLY = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/S3B/2024/CONUS_MO/ci_cyano_monthly_mean.csv\")\n",
    "P_LAKES_GPKG    = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\")\n",
    "P_LAKE_DAILY    = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/processed/lake_ci_daily.parquet\")\n",
    "P_LAKE_MONTHLY  = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/processed/lake_ci_monthly.parquet\")\n",
    "\n",
    "OUT_DIR = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/processed/qc\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "def _clip_series_q(s: pd.Series, qlow=CLIP_QUANTILE_LOW, qhigh=CLIP_QUANTILE_HIGH) -> pd.Series:\n",
    "    \"\"\"Clip a numeric series by quantiles; preserve NaNs.\"\"\"\n",
    "    if s.dropna().empty:\n",
    "        return s\n",
    "    lo = s.quantile(qlow)\n",
    "    hi = s.quantile(qhigh)\n",
    "    return s.clip(lower=lo, upper=hi)\n",
    "\n",
    "def _ensure_datetime(df: pd.DataFrame,\n",
    "                     prefer_cols=(\"date\", \"time\")) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust datetime parsing:\n",
    "    - try 'date' then 'time' (or any column that contains these names)\n",
    "    - tolerant to mixed formats; coerce to UTC tz-aware then drop tz\n",
    "    - drop NaT rows\n",
    "    \"\"\"\n",
    "    col = None\n",
    "    for cand in prefer_cols:\n",
    "        if cand in df.columns:\n",
    "            col = cand\n",
    "            break\n",
    "    if col is None:\n",
    "        for c in df.columns:\n",
    "            lc = str(c).lower()\n",
    "            if \"date\" in lc or \"time\" in lc or \"datetime\" in lc or \"timestamp\" in lc:\n",
    "                col = c\n",
    "                break\n",
    "    if col is None:\n",
    "        raise ValueError(\"No recognizable datetime column (expect 'date'/'time').\")\n",
    "\n",
    "    dt = pd.to_datetime(df[col], utc=True, format=\"mixed\", errors=\"coerce\")\n",
    "    dt = dt.dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = dt\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=[\"date\"]).reset_index(drop=True)\n",
    "    if len(df) < before:\n",
    "        print(f\"[QC] Dropped {before - len(df)} rows with unparseable datetime in column '{col}'\")\n",
    "    return df\n",
    "\n",
    "def _summarize_basic(df: pd.DataFrame, tag: str) -> pd.DataFrame:\n",
    "    \"\"\"Basic counts and missingness summary for quick logging.\"\"\"\n",
    "    out = {\n",
    "        \"tag\": tag,\n",
    "        \"rows\": len(df),\n",
    "        \"n_missing_CI_mean\": int(df[\"CI_mean\"].isna().sum()) if \"CI_mean\" in df.columns else None,\n",
    "        \"n_missing_CI_p90\":  int(df[\"CI_p90\"].isna().sum()) if \"CI_p90\" in df.columns else None,\n",
    "        \"n_missing_n_valid\": int(df[\"n_valid\"].isna().sum()) if \"n_valid\" in df.columns else None,\n",
    "    }\n",
    "    return pd.DataFrame([out])\n",
    "\n",
    "# ----------------------------\n",
    "# QC for regional (CONUS) time series\n",
    "# ----------------------------\n",
    "def qc_conus_timeseries(csv_path: Path, freq: str, out_prefix: str) -> None:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    expected = {\"CI_mean\", \"CI_p90\", \"n_valid\"}\n",
    "    missing = expected - set(df.columns)\n",
    "    if missing:\n",
    "        print(f\"[WARN] {csv_path.name} is missing columns: {missing}\")\n",
    "\n",
    "    # robust parse + drop NaT\n",
    "    df = _ensure_datetime(df, prefer_cols=(\"date\", \"time\"))\n",
    "\n",
    "    # normalize timestamps to start-of-period\n",
    "    if freq.upper() == \"D\":\n",
    "        df[\"date\"] = df[\"date\"].dt.floor(\"D\")\n",
    "    elif freq.upper() == \"MS\":\n",
    "        df[\"date\"] = df[\"date\"].dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "    else:\n",
    "        df[\"date\"] = df[\"date\"].dt.to_period(freq).dt.start_time\n",
    "\n",
    "    df = df.sort_values(\"date\").drop_duplicates(subset=[\"date\"]).reset_index(drop=True)\n",
    "\n",
    "    # build regular index over the observed span\n",
    "    idx = pd.date_range(df[\"date\"].min(), df[\"date\"].max(), freq=freq)\n",
    "    df = df.set_index(\"date\").reindex(idx).rename_axis(\"date\").reset_index()\n",
    "\n",
    "    # short-gap interpolation on CI columns (both directions)\n",
    "    for c in [\"CI_mean\", \"CI_p90\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].interpolate(\n",
    "                limit=INTERP_LIMIT_DAYS if freq.upper()==\"D\" else 1,\n",
    "                limit_direction=\"both\"\n",
    "            )\n",
    "\n",
    "    # robust quantile clipping\n",
    "    for c in [\"CI_mean\", \"CI_p90\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = _clip_series_q(df[c])\n",
    "\n",
    "    if \"n_valid\" in df.columns:\n",
    "        nv_thr = df[\"n_valid\"].quantile(0.05) if df[\"n_valid\"].notna().any() else 0\n",
    "        df[\"qc_low_coverage\"] = (df[\"n_valid\"] < nv_thr).astype(int)\n",
    "\n",
    "    out_csv = OUT_DIR / f\"{out_prefix}_clean.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    _summarize_basic(df, tag=f\"{out_prefix}\").to_csv(OUT_DIR / f\"{out_prefix}_summary.csv\", index=False)\n",
    "    print(f\"[OK] Saved regional QC → {out_csv}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Lake geometry → expected pixel count\n",
    "# ----------------------------\n",
    "def compute_expected_pixels_per_lake(lakes_gpkg: Path,\n",
    "                                     inset_m: float = 0.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute expected pixel counts per lake from geometry area, after optional inward buffer\n",
    "    (set inset_m=0 to avoid double-shrinking if polygons are already buffered).\n",
    "    Done in equal-area CRS (EPSG:5070) then back to 4326.\n",
    "\n",
    "    Returns a DataFrame: lake_id, lake_name (if present), expected_pixels_geom, area_m2\n",
    "    \"\"\"\n",
    "    g = gpd.read_file(lakes_gpkg)\n",
    "    cols = list(g.columns)\n",
    "    if \"lake_id\" not in cols:\n",
    "        raise ValueError(\"Expect 'lake_id' in lakes_greatlakes_5poly.gpkg\")\n",
    "\n",
    "    name_col = None\n",
    "    for cand in [\"lake_name\", \"Lake_name\", \"name\", \"Name\"]:\n",
    "        if cand in cols:\n",
    "            name_col = cand\n",
    "            break\n",
    "\n",
    "    gm = g.to_crs(5070)\n",
    "\n",
    "    if inset_m and inset_m > 0:\n",
    "        gm[\"geometry\"] = gm.buffer(-abs(inset_m))\n",
    "\n",
    "    gm[\"area_m2\"] = gm.geometry.area\n",
    "    gm[\"expected_pixels_geom\"] = (gm[\"area_m2\"] / (PIX_RES_M ** 2)).round().astype(\"Int64\")\n",
    "\n",
    "    df = gm.to_crs(4326)[[\"lake_id\", \"area_m2\", \"expected_pixels_geom\"]].copy()\n",
    "    if name_col:\n",
    "        df[name_col] = g[name_col].values\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# QC for lake-level time series\n",
    "# ----------------------------\n",
    "def _safe_nanmedian(arr) -> float:\n",
    "    \"\"\"Return NaN if all values are NaN or array empty; otherwise nanmedian.\"\"\"\n",
    "    a = pd.Series(arr).astype(float)\n",
    "    a = a[np.isfinite(a)]\n",
    "    return float(np.nanmedian(a)) if len(a) else float(\"nan\")\n",
    "\n",
    "def _coerce_n_valid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 1) 如果已有 n_valid 且存在非零值，直接返回\n",
    "    if \"n_valid\" in df.columns and (df[\"n_valid\"].fillna(0) > 0).any():\n",
    "        return df\n",
    "\n",
    "    # 2) 常见别名兜底\n",
    "    for alias in [\"valid_count\", \"count_valid\", \"num_valid\", \"n\", \"count\"]:\n",
    "        if alias in df.columns and (df[alias].fillna(0) > 0).any():\n",
    "            df = df.rename(columns={alias: \"n_valid\"})\n",
    "            return df\n",
    "\n",
    "    # 3) 若 n_valid 不存在或全为 0 → 改成 NaN，避免误触发覆盖率判定\n",
    "    if \"n_valid\" not in df.columns or not (df[\"n_valid\"].fillna(0) > 0).any():\n",
    "        df[\"n_valid\"] = np.nan\n",
    "        print(\"[WARN] n_valid is missing or all zeros in lake parquet; \"\n",
    "              \"set n_valid = NaN so coverage flags become NA.\")\n",
    "    return df\n",
    "\n",
    "def qc_lake_timeseries(parquet_path: Path, lakes_gpkg: Path, out_prefix: str) -> None:\n",
    "    \"\"\"\n",
    "    QC for lake-level CI time series (daily or monthly parquet).\n",
    "    Adds:\n",
    "      - expected_pixels based on lake geometry\n",
    "      - pct_valid_geom = n_valid / expected_pixels\n",
    "      - pct_valid_emp = n_valid / max(n_valid) per lake\n",
    "      - QC flags: low coverage (geom/emp), tiny absolute pixels\n",
    "    Outputs cleaned Parquet + summary CSV.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    df = _coerce_n_valid(df)\n",
    "\n",
    "    # Robust datetime parse: prefer 'date', then 'time'\n",
    "    df = _ensure_datetime(df, prefer_cols=(\"date\", \"time\"))\n",
    "\n",
    "    # Ensure single datetime column named 'date'\n",
    "    if \"time\" in df.columns and \"date\" in df.columns:\n",
    "        df = df.drop(columns=[\"time\"])\n",
    "    elif \"time\" in df.columns and \"date\" not in df.columns:\n",
    "        df = df.rename(columns={\"time\": \"date\"})\n",
    "\n",
    "    # Drop duplicated columns if any\n",
    "    if df.columns.duplicated().any():\n",
    "        dup = df.columns[df.columns.duplicated()].tolist()\n",
    "        print(f\"[QC] Dropping duplicated columns: {dup}\")\n",
    "        df = df.loc[:, ~df.columns.duplicated()].copy()\n",
    "\n",
    "    # Ensure a 'product' column exists\n",
    "    if \"product\" not in df.columns:\n",
    "        df[\"product\"] = \"unknown\"\n",
    "\n",
    "    # Expected pixels per lake from geometry (no extra inset here)\n",
    "    geom_df = compute_expected_pixels_per_lake(lakes_gpkg, inset_m=0.0)\n",
    "    name_col = \"lake_name\" if \"lake_name\" in geom_df.columns else (\"Lake_name\" if \"Lake_name\" in geom_df.columns else None)\n",
    "\n",
    "    # Merge expected pixels\n",
    "    df = df.merge(geom_df, on=\"lake_id\", how=\"left\")\n",
    "\n",
    "    # Empirical max n_valid per lake (skip zeros/NaNs)\n",
    "    if \"n_valid\" in df.columns:\n",
    "        emp_max = (\n",
    "            df.loc[df[\"n_valid\"].fillna(0) > 0]\n",
    "              .groupby(\"lake_id\")[\"n_valid\"]\n",
    "              .max()\n",
    "              .rename(\"empiric_n_valid_max\")\n",
    "        )\n",
    "        df = df.merge(emp_max, on=\"lake_id\", how=\"left\")\n",
    "\n",
    "        # Coverage metrics\n",
    "        df[\"pct_valid_geom\"] = df[\"n_valid\"] / df[\"expected_pixels_geom\"]\n",
    "        df[\"pct_valid_emp\"]  = df[\"n_valid\"] / df[\"empiric_n_valid_max\"]\n",
    "\n",
    "        for c in (\"pct_valid_geom\", \"pct_valid_emp\"):\n",
    "            df.loc[~np.isfinite(df[c]), c] = np.nan\n",
    "\n",
    "        # Coverage flags\n",
    "        df[\"qc_low_cov_geom\"] = pd.Series(pd.NA, index=df.index, dtype=\"Int64\")\n",
    "        ok_geom = df[\"pct_valid_geom\"].notna()\n",
    "        df.loc[ok_geom, \"qc_low_cov_geom\"] = (df.loc[ok_geom, \"pct_valid_geom\"] < MIN_PCT_VALID_GEOM).astype(\"Int64\")\n",
    "\n",
    "        df[\"qc_low_cov_emp\"] = pd.Series(pd.NA, index=df.index, dtype=\"Int64\")\n",
    "        ok_emp = df[\"pct_valid_emp\"].notna()\n",
    "        df.loc[ok_emp, \"qc_low_cov_emp\"] = (df.loc[ok_emp, \"pct_valid_emp\"] < MIN_PCT_VALID_EMP).astype(\"Int64\")\n",
    "\n",
    "        df[\"qc_tiny_abs_pix\"] = (pd.to_numeric(df[\"n_valid\"], errors=\"coerce\") < MIN_ABS_PIX).astype(\"Int64\")\n",
    "    else:\n",
    "        for c in [\"pct_valid_geom\", \"pct_valid_emp\", \"qc_low_cov_geom\", \"qc_low_cov_emp\", \"qc_tiny_abs_pix\"]:\n",
    "            df[c] = pd.NA\n",
    "\n",
    "    # Clip CI columns lake-wise (robust to per-lake distributions)\n",
    "    for c in [\"CI_mean\", \"CI_p90\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df.groupby(\"lake_id\", group_keys=False)[c].apply(\n",
    "                lambda s: _clip_series_q(s, CLIP_QUANTILE_LOW, CLIP_QUANTILE_HIGH)\n",
    "            )\n",
    "\n",
    "    # Consolidated validity flag\n",
    "    cov_flags = [\"qc_low_cov_geom\", \"qc_low_cov_emp\", \"qc_tiny_abs_pix\"]\n",
    "    for c in cov_flags:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "\n",
    "    df[\"qc_is_valid\"] = 1  # start as valid\n",
    "    for c in cov_flags:\n",
    "        df.loc[df[c] == 1, \"qc_is_valid\"] = 0\n",
    "\n",
    "    # rows with no coverage info → fall back to \"CI_mean is finite\"\n",
    "    rows_no_cov = df[cov_flags].isna().all(axis=1)\n",
    "    ci_ok = pd.to_numeric(df.get(\"CI_mean\"), errors=\"coerce\")\n",
    "    ci_ok = ci_ok.apply(lambda x: int(np.isfinite(x)))\n",
    "    df.loc[rows_no_cov, \"qc_is_valid\"] = ci_ok.loc[rows_no_cov].values\n",
    "\n",
    "    # Optional quick diagnostics (helps when a lake looks empty in plots)\n",
    "    diag = (\n",
    "        df.assign(CI_mean_finite = pd.to_numeric(df[\"CI_mean\"], errors=\"coerce\").apply(np.isfinite))\n",
    "          .groupby(\"lake_id\")\n",
    "          .agg(n_rows=(\"lake_id\",\"size\"),\n",
    "               n_CI_finite=(\"CI_mean_finite\",\"sum\"),\n",
    "               n_valid_pos=(\"n_valid\", lambda s: int((pd.to_numeric(s, errors=\"coerce\")>0).sum())),\n",
    "               qc_valid=(\"qc_is_valid\",\"sum\"))\n",
    "          .reset_index()\n",
    "    )\n",
    "    print(\"\\n[DIAG] per-lake counts:\\n\", diag.to_string(index=False))\n",
    "\n",
    "    # Save cleaned parquet\n",
    "    out_pq = OUT_DIR / f\"{out_prefix}_clean.parquet\"\n",
    "    df.to_parquet(out_pq, index=False)\n",
    "\n",
    "    # Build simple summary per-lake\n",
    "    summary_rows = []\n",
    "    for lid, g in df.groupby(\"lake_id\"):\n",
    "        n_rows = len(g)\n",
    "        n_valid_rows = int((g[\"qc_is_valid\"] == 1).sum())\n",
    "        frac_valid = n_valid_rows / n_rows if n_rows else 0.0\n",
    "        row = {\n",
    "            \"lake_id\": lid,\n",
    "            \"rows\": n_rows,\n",
    "            \"rows_valid\": n_valid_rows,\n",
    "            \"frac_valid\": round(frac_valid, 4),\n",
    "            \"pct_valid_geom_med\": _safe_nanmedian(g[\"pct_valid_geom\"]) if \"pct_valid_geom\" in g.columns else np.nan,\n",
    "            \"pct_valid_emp_med\":  _safe_nanmedian(g[\"pct_valid_emp\"])  if \"pct_valid_emp\"  in g.columns else np.nan,\n",
    "        }\n",
    "        if name_col and name_col in g.columns:\n",
    "            row[\"lake_name\"] = g[name_col].iloc[0]\n",
    "        summary_rows.append(row)\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows).sort_values(\"lake_id\")\n",
    "    out_summary = OUT_DIR / f\"{out_prefix}_summary.csv\"\n",
    "    summary_df.to_csv(out_summary, index=False)\n",
    "\n",
    "    print(f\"[OK] Saved lake-level QC → {out_pq}\")\n",
    "    print(f\"[OK] Summary → {out_summary}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Regional time series QC\n",
    "    if P_CONUS_DAILY.exists():\n",
    "        qc_conus_timeseries(P_CONUS_DAILY,   freq=\"D\",  out_prefix=\"conus_daily\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Missing: {P_CONUS_DAILY}\")\n",
    "\n",
    "    if P_CONUS_MONTHLY.exists():\n",
    "        qc_conus_timeseries(P_CONUS_MONTHLY, freq=\"MS\", out_prefix=\"conus_monthly\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Missing: {P_CONUS_MONTHLY}\")\n",
    "\n",
    "    # 2) Lake-level QC (Great Lakes)\n",
    "    if P_LAKE_DAILY.exists():\n",
    "        qc_lake_timeseries(P_LAKE_DAILY,  P_LAKES_GPKG, out_prefix=\"greatlakes_daily\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Missing: {P_LAKE_DAILY}\")\n",
    "\n",
    "    if P_LAKE_MONTHLY.exists():\n",
    "        qc_lake_timeseries(P_LAKE_MONTHLY, P_LAKES_GPKG, out_prefix=\"greatlakes_monthly\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Missing: {P_LAKE_MONTHLY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254a9d3",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "Available at independent python file `HAB-forecasting/data_visualization_ILW.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9fdb30",
   "metadata": {},
   "source": [
    "## Dataset: Daymet v4 Daily Surface Weather Data (ORNL DAAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13671ac",
   "metadata": {},
   "source": [
    "Read Great Lakes Polygon data (GPKG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa2065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pydaymet as daymet\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "GPKG = \"/dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\"\n",
    "\n",
    "lakes = gpd.read_file(GPKG)\n",
    "# 确保是经纬度坐标（EPSG:4326）；PyDaymet也支持其它CRS，传参loc_crs即可\n",
    "lakes = lakes.to_crs(4326)\n",
    "\n",
    "# 准备日期与变量清单\n",
    "# DATES = (\"2016-01-01\", \"2025-12-31\")\n",
    "DATES = (\"2024-01-01\", \"2024-12-31\")\n",
    "VARS  = [\"tmin\",\"tmax\",\"prcp\",\"srad\",\"vp\",\"dayl\"]\n",
    "\n",
    "# 如果没有湖名列，就造一个 id\n",
    "if \"name\" not in lakes.columns:\n",
    "    lakes[\"name\"] = [f\"lake_{i}\" for i in range(len(lakes))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19464b83",
   "metadata": {},
   "source": [
    "逐湖请求 Daymet 网格并做“湖面平均”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c158faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/Daymet/glakes_nc\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dfs_daily = []   # 存各湖“日尺度湖面平均”的表\n",
    "\n",
    "for idx, row in lakes.iterrows():\n",
    "    geom   = row.geometry\n",
    "    lakeid = row[\"name\"]\n",
    "\n",
    "    # 取 Daymet 栅格（daily）\n",
    "    ds = daymet.get_bygeom(\n",
    "        geom, DATES, variables=VARS,  # time_scale 默认 daily\n",
    "        # 如你的 GPKG 不是4326，可加 loc_crs=你的EPSG\n",
    "    )\n",
    "\n",
    "    # 按空间维度(y,x)做湖面平均；skipna 处理缺测\n",
    "    ds_mean = ds[VARS].mean(dim=[\"y\",\"x\"], skipna=True)\n",
    "\n",
    "    # 存 netCDF 以便复用\n",
    "    ds.to_netcdf(out_dir / f\"daymet_{lakeid}_daily.nc\")\n",
    "\n",
    "    # 转成 DataFrame（带时间索引）\n",
    "    df = ds_mean.to_dataframe().reset_index()\n",
    "    df = df.rename(columns={\"time\": \"date\"})\n",
    "    df[\"lake_id\"] = lakeid\n",
    "    dfs_daily.append(df)\n",
    "\n",
    "daymet_daily = pd.concat(dfs_daily, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef7955",
   "metadata": {},
   "source": [
    "与 ILW 时间步对齐：周均 / 28 天滑动均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先确保日期是 datetime\n",
    "daymet_daily[\"date\"] = pd.to_datetime(daymet_daily[\"date\"])\n",
    "\n",
    "# --- 周聚合（自然周，对齐周一/周日可改 label/closed） ---\n",
    "wk = (daymet_daily\n",
    "      .set_index(\"date\")\n",
    "      .groupby(\"lake_id\")\n",
    "      [VARS].resample(\"7D\").mean()\n",
    "      .reset_index()\n",
    "     )\n",
    "\n",
    "# --- 28天移动平均（滑窗，不跳步；末端可能因窗口不满而NA） ---\n",
    "daymet_daily = daymet_daily.sort_values([\"lake_id\",\"date\"])\n",
    "roll28 = (daymet_daily\n",
    "          .groupby(\"lake_id\")[VARS]\n",
    "          .rolling(\"28D\", on=daymet_daily[\"date\"])\n",
    "          .mean()\n",
    "          .reset_index()\n",
    "         )\n",
    "roll28 = roll28.rename(columns={\"level_1\":\"date\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e5fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd311e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
