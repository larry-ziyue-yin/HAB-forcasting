{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73043a68",
   "metadata": {},
   "source": [
    "# Forecasting Freshwater Algal Bloom Levels Using Multisource Climate and Water-Quality Data\n",
    "\n",
    "Course project of **STATS 402: Interdisciplinary Data Analysis**.\n",
    "\n",
    "**Name:** Ziyue Yin\n",
    "\n",
    "**NetID:** zy166"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e9d58b",
   "metadata": {},
   "source": [
    "## NASA OceanColor Inland Waters (ILW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafae03e",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d712f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading scripts are independet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0419fb8",
   "metadata": {},
   "source": [
    "After downloading the datasets, the structure should be shown as follows:\n",
    "\n",
    "```\n",
    "datasets/\n",
    " ├── ILW/\n",
    " │    ├── S3B/2024/CONUS_MO/\n",
    " │    │      ├── S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    │      ├── S3B_OLCI_EFRNT.20240201_20240229.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    ├── Merged/2024/CONUS_DAY/\n",
    " │    │      ├── S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    │      ├── S3M_OLCI_EFRNT.20240102.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    │      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e70cade",
   "metadata": {},
   "source": [
    "### Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e84bc5",
   "metadata": {},
   "source": [
    "First of all, let's glance at the monthly dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3545ae05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenMappingWarningOnValuesAccess({'y': 15138, 'x': 26328, 'rgb': 3, 'eightbitcolor': 256})\n",
      "['rhos_400', 'rhos_412', 'rhos_443', 'rhos_490', 'rhos_510', 'rhos_560', 'rhos_620', 'rhos_665', 'rhos_674', 'rhos_681', 'rhos_709', 'rhos_754', 'rhos_865', 'rhos_884', 'CI_cyano', 'palette']\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "p = \"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/S3B/2024/CONUS_MO/S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\"\n",
    "\n",
    "ds = xr.open_dataset(p, engine=\"netcdf4\", chunks=\"auto\")\n",
    "print(ds.dims)\n",
    "print(list(ds.data_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b94fb",
   "metadata": {},
   "source": [
    "And also, the daily dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a063b2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenMappingWarningOnValuesAccess({'y': 15138, 'x': 26328, 'rgb': 3, 'eightbitcolor': 256})\n",
      "['rhos_400', 'rhos_412', 'rhos_443', 'rhos_490', 'rhos_510', 'rhos_560', 'rhos_620', 'rhos_665', 'rhos_674', 'rhos_681', 'rhos_709', 'rhos_754', 'rhos_865', 'rhos_884', 'CI_cyano', 'palette']\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "p = \"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\"\n",
    "\n",
    "ds = xr.open_dataset(p, engine=\"netcdf4\", chunks=\"auto\")\n",
    "print(ds.dims)\n",
    "print(list(ds.data_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627b349",
   "metadata": {},
   "source": [
    "### Target Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edbba54",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8fe2bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import rioxarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb166a00",
   "metadata": {},
   "source": [
    "Time Extraction: coords -> attrs -> file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d9caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def infer_time_label(nc_path, ds, product=\"monthly\"):\n",
    "    \"\"\"\n",
    "    返回一个 pandas.Timestamp，尽量从 ds 或文件名推断。\n",
    "    product: 'monthly' or 'daily'\n",
    "    \"\"\"\n",
    "    # 1) 直接有 time 坐标/变量\n",
    "    for k in (\"time\",):\n",
    "        if k in ds.coords or k in ds.variables:\n",
    "            try:\n",
    "                return pd.to_datetime(ds[k].values).to_pydatetime()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # 2) 全局属性（常见于 L3m）\n",
    "    start = ds.attrs.get(\"time_coverage_start\") or ds.attrs.get(\"start_time\")\n",
    "    end   = ds.attrs.get(\"time_coverage_end\")   or ds.attrs.get(\"end_time\")\n",
    "    if start and end:\n",
    "        try:\n",
    "            ts = pd.to_datetime(start)\n",
    "            te = pd.to_datetime(end)\n",
    "            # 月度：常用“月末”或“中点”；这里给你“中点”更通用\n",
    "            if product == \"monthly\":\n",
    "                return ts + (te - ts) / 2\n",
    "            else:\n",
    "                # 日产品：用开始时间\n",
    "                return ts\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 3) 从文件名解析\n",
    "    fn = nc_path.split(\"/\")[-1]\n",
    "    if product == \"monthly\":\n",
    "        # ...YYYYMMDD_YYYYMMDD.L3m.MO...\n",
    "        m = re.search(r\"\\.(\\d{8})_(\\d{8})\\.L3m\\.MO\\.\", fn)\n",
    "        if m:\n",
    "            b, e = m.group(1), m.group(2)\n",
    "            ts = pd.to_datetime(b, format=\"%Y%m%d\")\n",
    "            te = pd.to_datetime(e, format=\"%Y%m%d\")\n",
    "            return ts + (te - ts) / 2\n",
    "    else:\n",
    "        # ...YYYYMMDD.L3m.DAY...\n",
    "        m = re.search(r\"\\.(\\d{8})\\.L3m\\.DAY\\.\", fn)\n",
    "        if m:\n",
    "            return pd.to_datetime(m.group(1), format=\"%Y%m%d\")\n",
    "\n",
    "    # 实在没有，就抛异常，提示检查文件\n",
    "    raise ValueError(\"Cannot infer time from dataset or filename: \" + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e9e64",
   "metadata": {},
   "source": [
    "Quality Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119622ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ci(da: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    依据变量属性做物理范围过滤，并去掉接近下界的“近零”点。\n",
    "    \"\"\"\n",
    "    vmin = float(da.attrs.get(\"valid_min\", np.nan))\n",
    "    vmax = float(da.attrs.get(\"valid_max\", np.nan))\n",
    "    if np.isfinite(vmin):\n",
    "        da = da.where(da >= vmin)\n",
    "    if np.isfinite(vmax):\n",
    "        da = da.where(da <= vmax)\n",
    "\n",
    "    # 去掉接近下界的小值（阈值可按需要调整）\n",
    "    thr = max(vmin, 5e-5) if np.isfinite(vmin) else 5e-5\n",
    "    da = da.where(da > thr)\n",
    "\n",
    "    return da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435bbce3",
   "metadata": {},
   "source": [
    "For a single .nc file, get all the lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lakes_from_nc(nc_path: str,\n",
    "                          lakes_gdf: gpd.GeoDataFrame,\n",
    "                          lake_id_col: str,\n",
    "                          product: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    nc_path: 单个 NetCDF 文件（S3B monthly 或 S3M daily）\n",
    "    lakes_gdf: 包含 lake_id 和 geometry 的 GeoDataFrame（EPSG:4326）\n",
    "    product: 'monthly' | 'daily'\n",
    "    返回：每个湖一行（该文件的时间戳）\n",
    "    \"\"\"\n",
    "    ds = xr.open_dataset(nc_path, engine=\"netcdf4\", chunks=\"auto\")\n",
    "    t  = infer_time_label(nc_path, ds, product=product)\n",
    "\n",
    "    da = ds[\"CI_cyano\"]\n",
    "    da = set_spatial_dims_safe(da)\n",
    "    da = clean_ci(da)\n",
    "\n",
    "    rows = []\n",
    "    for _, row in lakes_gdf.iterrows():\n",
    "        lid  = row[lake_id_col]\n",
    "        geom = [row.geometry]  # rioxarray.clip 需要 list\n",
    "\n",
    "        try:\n",
    "            clipped = da.rio.clip(geom, lakes_gdf.crs, drop=True)\n",
    "            valid   = clipped.where(np.isfinite(clipped))\n",
    "            n_valid = int(valid.count().compute().values)\n",
    "            if n_valid == 0:\n",
    "                mean_val = np.nan\n",
    "                p90      = np.nan\n",
    "            else:\n",
    "                mean_val = float(valid.mean().compute().values)\n",
    "                p90      = float(valid.quantile(0.9).compute().values)\n",
    "        except Exception:\n",
    "            mean_val, p90, n_valid = np.nan, np.nan, 0\n",
    "\n",
    "        rows.append({\n",
    "            \"lake_id\": lid,\n",
    "            \"time\":   pd.to_datetime(t),\n",
    "            \"product\": product,\n",
    "            \"CI_mean\": mean_val,\n",
    "            \"CI_p90\":  p90,\n",
    "            \"n_valid\": n_valid,\n",
    "            \"src\":     Path(nc_path).name,\n",
    "        })\n",
    "\n",
    "    ds.close()\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d60dc4",
   "metadata": {},
   "source": [
    "Process monthly data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84adfb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_monthly(monthly_dir: str,\n",
    "                lakes_fp: str,\n",
    "                lake_id_col: str,\n",
    "                out_parquet: str):\n",
    "    \"\"\"\n",
    "    monthly_dir: 目录内文件形如 S3B_OLCI_EFRNT.*.L3m.MO.ILW_CONUS...nc\n",
    "    lakes_fp:    湖泊边界（gpkg/shp，需 EPSG:4326）\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(lakes_fp)\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"湖泊文件缺少 CRS，请确保为 EPSG:4326\")\n",
    "    gdf = gdf.to_crs(4326)[[lake_id_col, \"geometry\"]].dropna()\n",
    "\n",
    "    out_rows = []\n",
    "    for fp in sorted(Path(monthly_dir).glob(\"S3B_OLCI_EFRNT.*.L3m.MO.*.nc\")):\n",
    "        df_one = extract_lakes_from_nc(str(fp), gdf, lake_id_col, product=\"monthly\")\n",
    "        out_rows.append(df_one)\n",
    "\n",
    "    if not out_rows:\n",
    "        print(\"No monthly files found.\")\n",
    "        return\n",
    "\n",
    "    df_all = pd.concat(out_rows, ignore_index=True)\n",
    "    Path(out_parquet).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_all.to_parquet(out_parquet, index=False)\n",
    "    print(f\"[monthly] saved → {out_parquet}  ({len(df_all)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a17ebf",
   "metadata": {},
   "source": [
    "Process daily data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b24609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_daily(daily_dir: str,\n",
    "              lakes_fp: str,\n",
    "              lake_id_col: str,\n",
    "              out_parquet: str):\n",
    "    \"\"\"\n",
    "    daily_dir: 目录内文件形如 S3M_OLCI_EFRNT.*.L3m.DAY.ILW_CONUS...nc\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(lakes_fp)\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"湖泊文件缺少 CRS，请确保为 EPSG:4326\")\n",
    "    gdf = gdf.to_crs(4326)[[lake_id_col, \"geometry\"]].dropna()\n",
    "\n",
    "    out_rows = []\n",
    "    for fp in sorted(Path(daily_dir).glob(\"S3M_OLCI_EFRNT.*.L3m.DAY.*.nc\")):\n",
    "        df_one = extract_lakes_from_nc(str(fp), gdf, lake_id_col, product=\"daily\")\n",
    "        out_rows.append(df_one)\n",
    "\n",
    "    if not out_rows:\n",
    "        print(\"No daily files found.\")\n",
    "        return\n",
    "\n",
    "    df_all = pd.concat(out_rows, ignore_index=True)\n",
    "    Path(out_parquet).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_all.to_parquet(out_parquet, index=False)\n",
    "    print(f\"[daily] saved → {out_parquet}  ({len(df_all)} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df32d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b904d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01691f18",
   "metadata": {},
   "source": [
    "Spatial Coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3e57e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_spatial_dims_safe(da: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    尝试为 L3m 网格设置 rioxarray 所需的空间维和 CRS。\n",
    "    先使用 x/y；失败则尝试 lon/lat；再不行退化为最后两个维度命名为 x/y。\n",
    "    \"\"\"\n",
    "    if \"x\" in da.dims and \"y\" in da.dims:\n",
    "        out = da.rio.write_crs(4326)\n",
    "        out = out.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "        return out\n",
    "\n",
    "    if \"lon\" in da.dims and \"lat\" in da.dims:\n",
    "        out = da.rio.write_crs(4326)\n",
    "        out = out.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=False)\n",
    "        return out\n",
    "\n",
    "    # 退化方案：尝试把最后两个维度当作 x/y\n",
    "    if len(da.dims) >= 2:\n",
    "        dims = list(da.dims)\n",
    "        ydim, xdim = dims[-2], dims[-1]\n",
    "        out = da.rename({xdim: \"x\", ydim: \"y\"})\n",
    "        out = out.rio.write_crs(4326)\n",
    "        out = out.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "        return out\n",
    "\n",
    "    raise ValueError(\"Cannot determine spatial dims for CI_cyano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000fcc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4665a61",
   "metadata": {},
   "source": [
    "#### Scale 1: In general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f52fe",
   "metadata": {},
   "source": [
    "##### Monthly\n",
    "\n",
    "Here, we use the **S3B Monthly** data. One month per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f98a86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>CI_mean</th>\n",
       "      <th>CI_p90</th>\n",
       "      <th>n_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-16 16:53:28.500000+00:00</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>9965844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-15 17:12:11.500000+00:00</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>10102417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-16 16:44:09+00:00</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>9452529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-16 04:52:02+00:00</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>10500573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-05-16 17:01:29+00:00</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>11822670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              time   CI_mean   CI_p90   n_valid\n",
       "0 2024-01-16 16:53:28.500000+00:00  0.000160  0.00005   9965844\n",
       "1 2024-02-15 17:12:11.500000+00:00  0.000157  0.00005  10102417\n",
       "2        2024-03-16 16:44:09+00:00  0.000135  0.00005   9452529\n",
       "3        2024-04-16 04:52:02+00:00  0.000141  0.00005  10500573\n",
       "4        2024-05-16 17:01:29+00:00  0.000149  0.00005  11822670"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob, numpy as np, pandas as pd, xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "monthly_dir = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/S3B/2024/CONUS_MO\")\n",
    "out_csv = monthly_dir/\"ci_cyano_monthly_mean.csv\"\n",
    "\n",
    "rows = []\n",
    "for fp in sorted(monthly_dir.glob(\"S3B_OLCI_EFRNT.*.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\")):\n",
    "    ds = xr.open_dataset(fp, engine=\"netcdf4\", chunks=\"auto\")\n",
    "    da = ds[\"CI_cyano\"]\n",
    "\n",
    "    vmin = float(da.attrs.get(\"valid_min\", np.nan))\n",
    "    vmax = float(da.attrs.get(\"valid_max\", np.nan))\n",
    "    if np.isfinite(vmin): da = da.where(da >= vmin)\n",
    "    if np.isfinite(vmax): da = da.where(da <= vmax)\n",
    "\n",
    "    m   = float(da.where(np.isfinite(da)).mean().compute().values)\n",
    "    p90 = float(da.where(np.isfinite(da)).quantile(0.9).compute().values)\n",
    "    t   = infer_time_label(str(fp), ds, product=\"monthly\")\n",
    "\n",
    "    rows.append({\"time\": pd.to_datetime(t), \"CI_mean\": m, \"CI_p90\": p90,\n",
    "                 \"n_valid\": int(da.count().compute().values)})\n",
    "    ds.close()\n",
    "\n",
    "df_mo = pd.DataFrame(rows).sort_values(\"time\").reset_index(drop=True)\n",
    "df_mo.to_csv(out_csv, index=False)\n",
    "df_mo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b93b8d",
   "metadata": {},
   "source": [
    "##### Daily\n",
    "\n",
    "Here, we use the **S3M Daily** data. One day per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1970169f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>CI_mean</th>\n",
       "      <th>CI_p90</th>\n",
       "      <th>n_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 13:52:31+00:00</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>5055519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02 13:36:49+00:00</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>5360242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03 14:02:06+00:00</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>4457855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04 13:40:26+00:00</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>5619796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-05 13:48:41+00:00</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>5554313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date   CI_mean   CI_p90  n_valid\n",
       "0 2024-01-01 13:52:31+00:00  0.000127  0.00005  5055519\n",
       "1 2024-01-02 13:36:49+00:00  0.000094  0.00005  5360242\n",
       "2 2024-01-03 14:02:06+00:00  0.000112  0.00005  4457855\n",
       "3 2024-01-04 13:40:26+00:00  0.000087  0.00005  5619796\n",
       "4 2024-01-05 13:48:41+00:00  0.000096  0.00005  5554313"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_dir = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/Merged/2024/CONUS_DAY\")\n",
    "out_csv = daily_dir/\"ci_cyano_daily_mean.csv\"\n",
    "\n",
    "rows = []\n",
    "for fp in sorted(daily_dir.glob(\"S3M_OLCI_EFRNT.*.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\")):\n",
    "    ds = xr.open_dataset(fp, engine=\"netcdf4\", chunks=\"auto\")\n",
    "    da = ds[\"CI_cyano\"]\n",
    "\n",
    "    vmin = float(da.attrs.get(\"valid_min\", np.nan))\n",
    "    vmax = float(da.attrs.get(\"valid_max\", np.nan))\n",
    "    if np.isfinite(vmin): da = da.where(da >= vmin)\n",
    "    if np.isfinite(vmax): da = da.where(da <= vmax)\n",
    "\n",
    "    m   = float(da.where(np.isfinite(da)).mean().compute().values)\n",
    "    p90 = float(da.where(np.isfinite(da)).quantile(0.9).compute().values)\n",
    "    t   = infer_time_label(str(fp), ds, product=\"daily\")\n",
    "\n",
    "    rows.append({\"date\": pd.to_datetime(t), \"CI_mean\": m, \"CI_p90\": p90,\n",
    "                 \"n_valid\": int(da.count().compute().values)})\n",
    "    ds.close()\n",
    "\n",
    "df_day = pd.DataFrame(rows).sort_values(\"date\").reset_index(drop=True)\n",
    "df_day.to_csv(out_csv, index=False)\n",
    "df_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc2c7b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== /dkucc/home/zy166/HAB-forcasting/datasets/ILW/S3B/2024/CONUS_MO/S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\n",
      "dims: ('y', 'x') Frozen({'y': 15138, 'x': 26328})\n",
      "attrs: {'valid_min': np.float32(5e-05), 'valid_max': np.float32(0.05), '_FillValue': None, 'scale_factor': None, 'add_offset': None}\n",
      "raw quantiles: [4.99999805e-05 4.99999951e-05 4.99999987e-05 5.00000024e-05\n",
      " 7.23447204e-02]\n",
      "\n",
      "== /dkucc/home/zy166/HAB-forcasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\n",
      "dims: ('y', 'x') Frozen({'y': 15138, 'x': 26328})\n",
      "attrs: {'valid_min': np.float32(5e-05), 'valid_max': np.float32(0.05), '_FillValue': None, 'scale_factor': None, 'add_offset': None}\n",
      "raw quantiles: [4.99999915e-05 4.99999951e-05 4.99999987e-05 5.00000024e-05\n",
      " 8.10017735e-02]\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr, numpy as np, pandas as pd, glob, re\n",
    "\n",
    "# 任选一个月度/日度文件\n",
    "f_mo = sorted(glob.glob(\"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/S3B/2024/CONUS_MO/S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\"))[0]\n",
    "f_da = sorted(glob.glob(\"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\"))[0]\n",
    "\n",
    "for f in [f_mo, f_da]:\n",
    "    ds = xr.open_dataset(f, engine=\"netcdf4\", chunks=\"auto\")\n",
    "    da = ds[\"CI_cyano\"]\n",
    "    print(\"\\n==\", f)\n",
    "    print(\"dims:\", da.dims, da.sizes)\n",
    "    print(\"attrs:\", {k: da.attrs.get(k) for k in [\"valid_min\",\"valid_max\",\"_FillValue\",\"scale_factor\",\"add_offset\"]})\n",
    "    # 统计分布（不裁剪，看看原始情况）\n",
    "    s = da.load().values.flatten()\n",
    "    s = s[np.isfinite(s)]\n",
    "    q = np.quantile(s, [0.0, 0.1, 0.5, 0.9, 1.0])\n",
    "    print(\"raw quantiles:\", q)\n",
    "    ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4fc325",
   "metadata": {},
   "source": [
    "#### Scale 2: Specific Lake(s)/Area(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b1dc42",
   "metadata": {},
   "source": [
    "import geopandas as gpd, rioxarray\n",
    "shp = gpd.read_file(\"shapes/lakes_conus.gpkg\")  # 每个湖 polygon 带 lake_id\n",
    "ds = xr.open_dataset(\".../S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\")\n",
    "\n",
    "da = ds['CI_cyano'].rio.write_crs(\"EPSG:4326\")\n",
    "for _, lake in shp.iterrows():\n",
    "    mask = da.rio.clip([lake.geometry], drop=True)\n",
    "    mean_val = float(mask.where(mask.notnull()).mean().values)\n",
    "    print(lake['lake_id'], mean_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25773dfe",
   "metadata": {},
   "source": [
    "### Time Alignment & Missing Value Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f4ba19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ead180bf",
   "metadata": {},
   "source": [
    "### Standard Preprocessed Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a1a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c80f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1620068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e31020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
